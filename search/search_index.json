{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About the workshop","text":"<p>This Workshop uses Cloud Pak for Data version 4.7</p>"},{"location":"#analyzing-credit-risk-with-cloud-pak-for-data-on-openshift","title":"Analyzing Credit Risk with Cloud Pak for Data on OpenShift","text":"<p>Welcome to our workshop! In this workshop we'll be using the Cloud Pak for Data platform to Collect Data, Organize Data, Analyze Data, and Infuse AI into our applications. The goals of this workshop are:</p> <ul> <li>Collect and virtualize data</li> <li>Visualize data with Data Refinery</li> <li>Create and deploy a machine learning model</li> <li>Monitor the model</li> <li>Create a Python app to use the model</li> </ul>"},{"location":"#about-this-workshop","title":"About this workshop","text":"<ul> <li>Agenda</li> <li>Compatability</li> <li>About Cloud Pak for Data</li> <li>Credits</li> </ul>"},{"location":"#about-the-data-set","title":"About the data set","text":"<p>In this workshop we will be using a credit risk / lending scenario. In this scenario, lenders respond to an increased pressure to expand lending to larger and more diverse audiences, by using different approaches to risk modeling. This means going beyond traditional credit data sources to alternative credit sources (i.e. mobile phone plan payment histories, education, etc), which may introduce risk of bias or other unexpected correlations.</p> <p></p> <p>The credit risk model that we are exploring in this workshop uses a training data set that contains 20 attributes about each loan applicant. The scenario and model use synthetic data based on the UCI German Credit dataset. The data is split into three CSV files and are located in the data directory of the GitHub repository you will download in the pre-work section.</p>"},{"location":"#applicant-financial-data","title":"Applicant Financial Data","text":"<p>This file has the following attributes:</p> <ul> <li>CUSTOMERID (hex number, used as Primary Key)</li> <li>CHECKINGSTATUS</li> <li>CREDITHISTORY</li> <li>EXISTINGSAVINGS</li> <li>INSTALLMENTPLANS</li> <li>EXISTINGCREDITSCOUNT</li> </ul>"},{"location":"#applicant-loan-data","title":"Applicant Loan Data","text":"<p>This file has the following attributes:</p> <ul> <li>CUSTOMERID</li> <li>LOANDURATION</li> <li>LOANPURPOSE</li> <li>LOANAMOUNT</li> <li>INSTALLMENTPERCENT</li> <li>OTHERSONLOAN</li> <li>RISK</li> </ul>"},{"location":"#applicant-personal-data","title":"Applicant Personal Data","text":"<p>This file has the following attributes:</p> <ul> <li>CUSTOMERID</li> <li>EMPLOYMENTDURATION</li> <li>SEX</li> <li>CURRENTRESIDENCEDURATION</li> <li>OWNSPROPERTY</li> <li>AGE</li> <li>HOUSING</li> <li>JOB</li> <li>DEPENDENTS</li> <li>TELEPHONE</li> <li>FOREIGNWORKER</li> <li>FIRSTNAME</li> <li>LASTNAME</li> <li>EMAIL</li> <li>STREETADDRESS</li> <li>CITY</li> <li>STATE</li> <li>POSTALCODE</li> </ul>"},{"location":"#agenda","title":"Agenda","text":"00:05 Welcome Welcome to the Cloud Pak for Data workshop 00:20 Lecture - Intro and Overview Introduction to Cloud Pak for Data and an Overview of this workshop 00:20 Lab - Pre-work Clone or Download the repo, create a project, create a deployment space 00:10 Walkthrough - Pre-work Clone or Download the repo, create a project, create a deployment space 00:20 Lecture - Data Refinery and Data Virtualization Data Refinery and Data Virtualization 00:30 Lab - Data Connection and Virtualization and importing the data into the project Creating a new connection, virtualizing the data, importing the data into the project 00:10 Walkthrough - Data Connection and Virtualization Creating a new connection, virtualizing the data, importing the data into the project 00:15 Lab - Import Data into Project Importing data in your projects 00:05 Walkthrough - Import Data into Project Importing data in your projects 00:15 Lab - Data Visualization with Data Refinery Refining the data, visualizing and profiling the data 00:10 Walkthrough - Data Visualization with Data Refinery Refining the data, visualizing and profiling the data 00:15 Lecture - Watson Knowledge Catalog Enterprise governance with Watson Knowledge Catalog 00:20 Lab - Enterprise data governance for Viewers using Watson Knowledge Catalog Use and Enterprise data catalog to search, manage, and protect data 00:05 Walkthrough - Enterprise data governance for Viewers using Watson Knowledge Catalog Use and Enterprise data catalog to search, manage, and protect data 00:20 Lab - Enterprise data governance for Admins using Watson Knowledge Catalog Create new Categories, Business terms, Policies and Rules in Watson Knowledge Catalog 00:05 Walkthrough - Enterprise data governance for Admins using Watson Knowledge Catalog Create new Categories, Business terms, Policies and Rules in Watson Knowledge Catalog 00:15 Lecture - Machine Learning Machine Learning and Deep Learning concepts 00:20 Lab - Machine Learning with Jupyter Building a model with Spark, deploying the model with Watson Machine Learning, testing the model with a Python Flask app 00:10 Walkthrough - Machine Learning with Jupyter Building a model with Spark, deploying the model with Watson Machine Learning, testing the model with a Python Flask app 00:20 Lab - AutoAI - Machine Learning with AutoAI Use AutoAi to quickly generate a Machine Learning pipeline and model 00:10 Walkthrough - Machine Learning with AutoAI Use AutoAi to quickly generate a Machine Learning pipeline and model 00:30 Lab - Deploy and Test Machine Learning Models Deploy and machine learning models using several approaches 00:10 Walkthrough - Deploy and Test Machine Learning Models Deploy and machine learning models using several approaches 00:15 Lab - Monitoring models with OpenScale GUI (Auto setup Monitoring) Quickly deploy an OpenScale demo with Auto setup 00:10 Walkthrough - Monitoring models with OpenScale GUI (Auto setup Monitoring) Quickly deploy an OpenScale demo with Auto setup 00:30 Lab - Monitoring models with OpenScale (Notebook) See the OpenScale APIs in a Jupyter notebook and manually configure the monitors 00:10 Walkthrough -  Monitoring models with OpenScale (Notebook) See the OpenScale APIs in a Jupyter notebook and manually configure the monitors 00:10 Closing Other capabilities, review, and next steps"},{"location":"#compatability","title":"Compatability","text":"<p>This workshop has been tested on the following platforms:</p> <ul> <li>macOS: Mojave (10.14), Catalina (10.15)</li> <li> <p>Google Chrome version 81</p> </li> <li> <p>Microsoft: Windows 10</p> </li> <li>Google Chrome, Microsoft Edge</li> </ul>"},{"location":"SUMMARY/","title":"WARNING: This repository is no longer maintained","text":"<p>This repository does not have active maintainers. Pull requests for fixes and enhancements will still be accepted, but no active work will be done on this workshop.</p> <p>This Workshop uses Cloud Pak for Data version 3.5</p>"},{"location":"SUMMARY/#summary","title":"Summary","text":""},{"location":"SUMMARY/#getting-started","title":"Getting Started","text":"<ul> <li>Pre-work</li> </ul>"},{"location":"SUMMARY/#credit-risk-workshop","title":"Credit Risk Workshop","text":"<ul> <li>Data Connection and Virtualization</li> </ul> <ul> <li>Import Data to Project</li> <li>Data Visualization with Data Refinery</li> <li>Enterprise data governance for Viewers using Watson Knowledge Catalog</li> <li>Enterprise data governance for Admins using Watson Knowledge Catalog</li> <li>Machine Learning with Jupyter</li> <li>Machine Learning with AutoAI</li> <li>Deploy and Test Machine Learning Models</li> <li>Monitoring models with OpenScale GUI (Auto setup Monitoring)</li> <li>Monitoring models with OpenScale (Notebook)</li> </ul>"},{"location":"SUMMARY/#workshop-resources","title":"Workshop Resources","text":"<ul> <li>FAQs / Tips</li> </ul>"},{"location":"SUMMARY/#resources","title":"Resources","text":"<ul> <li>IBM Cloud Pak for Data - Information and Trial</li> <li>IBM Cloud Pak for Data - Knowledge Center</li> <li>IBM Cloud Pak for Data - Platform API</li> <li>IBM Cloud Pak for Data - Community</li> <li>Watson Knowledge Catalog</li> <li>Watson Knowledge Catalog Learning Center</li> <li>IBM Developer</li> <li>IBM Developer - Cloud Pak for Data</li> <li>IBM Garage Architecture - Data</li> </ul>"},{"location":"addData/","title":"Importing data in your projects","text":"<p>There are many ways to bring your data into your project, in this section we'll cover using from the following data sources:</p> <ul> <li>Using virtualized data</li> </ul> <p>Note: The lab instructions below assume you have your project already available. If not, follow the instructions in the pre-work section to create a project.</p>"},{"location":"addData/#importing-virtualized-data","title":"Importing Virtualized Data","text":"<p>For this section we'll explore the data that is available from the virtualized views that have been created in Data Virtualization. This data may come from various data sources or tables within a single source, but will appear as a single data asset. We will add this data to an analytics project so that it can be used in subsequent modules of this workshop (i.e for data analysis and to build ML models).</p>"},{"location":"addData/#assign-the-data-to-your-project","title":"Assign the data to your project","text":"<ul> <li>To launch the data virtualization tool, go the (\u2630) navigation menu and under the <code>Data</code> section click on <code>Data virtualization</code>.</li> </ul> <ul> <li>From the data virtualization sub-menu at the top left of the page, click on the menu drop down list and choose <code>My virtualized data</code>. Here you should see the data you have virtualized or that you have been given access to (or that the administrator has assigned to you).</li> </ul> <ul> <li>Select the checkbox next to the data sets you want to use in your project and click the <code>Assign</code> button to start importing it to your project.</li> </ul> <p>Note: The name of the data assets to select may vary based on names chosen during data virtualization. The default names to select are: LOANS, APPLICANTFINANCIALDATA, APPLICANTPERSONALDATA, XXXAPPLICANTFINANCIALPERSONALDATA AND XXXAPPLICANTFINANCIALPERSONALLOANDATA</p> <p></p> <ul> <li>In the 'Assign virtual objects' panel, choose your project(s) from the drop-down list. Then click the <code>Assign</code> button to add the data to your project.</li> </ul> <p></p> <ul> <li>Once completed, you will receive a confirmation notification at the top of the page.</li> </ul> <p></p> <ul> <li>Go to your project by selecting the (\u2630) navigation menu and under the <code>Projects</code> section click on <code>All projects</code>.</li> </ul> <p></p> <ul> <li>Click on the name of the project where the data was assigned in the previous step to open it.</li> </ul> <p></p> <ul> <li>In the project page, clicking on the <code>Assets</code> tab will show the virtualized tables and joined tables that are now in your project (along with other assets that are in the project).</li> </ul> <p></p> <p>Do not go to the next section until you see the data assets in your project.</p>"},{"location":"addData/#conclusion","title":"Conclusion","text":"<p>This lab shows just one of the ways to gather data for your analytics projects in Cloud Pak for Data. In this case you used data that was previously virtualized. Other ways might include:  - browsing the catalogs - importing flat files - importing data from connections directly in the project</p>"},{"location":"data-connection-and-virtualization/","title":"Data Connection and Virtualization","text":"<p>This section will cover aspects of collecting data in Cloud Pak for Data. Specifically we will be connecting to different data sources and creating views against those data sources to create a single unified set of data assets that can be used in other modules of this workshop.</p> <p>Note: To complete this section, an Admin or Data Engineer role needs to be assigned to your user account. The workshop instructor will assign this role as appropriate.</p>"},{"location":"data-connection-and-virtualization/#virtualizing-data","title":"Virtualizing Data","text":"<p>In this section, we will gather data from several tables across data sources. We will use data virtualization to access these tables and then create joined views against those virtualized tables.</p>"},{"location":"data-connection-and-virtualization/#create-virtualized-tables","title":"Create Virtualized Tables","text":"<ul> <li>To launch the data virtualization tool, go the (\u2630) navigation menu and under the <code>Data</code> section click on <code>Data virtualization</code>.</li> </ul> <ul> <li>From the Data virtualization sub-menu at the top left of the page, click on the menu drop down list and choose <code>Virtualize</code>.</li> </ul> <ul> <li>Change from the <code>Explore</code> view to the <code>List</code> view to see all the available tables across the different data sources.</li> </ul> <ul> <li> <p>Several tables names will be displayed across any of the data sources that are included in the data virtualization server. You will notice that on the top of the panel, we can filter the tables being displayed by selecting the database type.</p> </li> <li> <p>To simplify the search for tables you will use, we will filter the list to one schema. Click on the <code>Schema</code> dropdown and select the <code>CP4DCREDIT</code> schema. click on the <code>Schemas</code> column header to sort the tables by Schema. </p> </li> </ul> <p></p> <ul> <li>Three tables should be displayed: <code>APPLICANTFINANCIALDATA</code>, <code>APPLICANTPERSONALDATA</code> and <code>LOANS</code>. Select the checkboxes next to these three tables, and then click on <code>Add to cart</code> followed by the <code>View Cart</code> button.</li> </ul> <p></p> <ul> <li>The next panel prompts you to select where to assign the virtualized tables. Click the <code>Virtualize</code> button to add the virtualized tables to your data (we left the default values, so the tables will be virtualized under your own user schema with the same table names as the original tables).</li> </ul> <p></p> <ul> <li>A pop up dialog panel will indicate that the virtual tables have been created. Let's see the new virtualized tables by clicking the <code>Go to virtualized data</code> button.</li> </ul> <p></p> <p>Note: You may receive a notification at the top of the page that the virtual assets were published to the catalog. Feel free to dismiss the notification by clicking on the <code>X</code></p>"},{"location":"data-connection-and-virtualization/#create-joined-virtual-views","title":"Create Joined Virtual Views","text":"<p>Now we're going to join the tables we previously virtualized, so we have a final merged set of data. It will be easier to do it here rather than in a notebook where we'd have to write code to handle three different data sets.</p> <ul> <li>Open the 'Virtualized data' page by clicking on the dropdown arrow and selecting <code>Virtualized data</code>. (You may already be there if you selected <code>Go to virtualized data</code> in the previous step.)</li> </ul> <p></p> <ul> <li>From the 'Virtualized data' page, click on two of the virtualized tables (<code>APPLICANTPERSONALDATA</code> and <code>APPLICANTFINANCIALDATA</code>) and click the <code>Join</code> button.</li> </ul> <p></p> <ul> <li>Click the checkboxes at the top of each table to select all columns for the join. Next, to join the tables we need to pick a key that is common to both data sets. Here we choose to map <code>CustomerID</code> from the first table to <code>CustomerID</code> on the second table. Do this by clicking on one and dragging it to another. When the line is drawn, click on the <code>Next</code> button.</li> </ul> <p></p> <p>Note: The blue line between the joined columns may disappear. The 'Join keys' view on the right-hand side should still contain the join definition, even if the line disappears.</p> <ul> <li>In the next panel, although we could change the names of our columns, we will accept the existing names for our columns. Click the <code>Next</code> button to continue.</li> </ul> <p></p> <ul> <li>In the next panel we'll give our joined data view a unique name (to be consistent with SQL standards, pick an all uppercase name), choose something like: <code>XXXAPPLICANTFINANCIALPERSONALDATA</code> (where <code>XXX</code> is your initials in all upper case) then click the <code>Create view</code> button to add the virtualized aggregate view to your data.</li> </ul> <p></p> <ul> <li>A pop up dialog panel will indicate that the join view creation has succeeded! Click on <code>Go to virutalized data</code> button.</li> </ul> <p></p> <ul> <li>Repeat the same steps as above, but this time choose to join the new joined view you just created (<code>XXXAPPLICANTFINANCIALPERSONALDATA</code>) and the last virtualized table (<code>LOANS</code>) to create a new joined view that has all three tables. Click the <code>Join</code> button.</li> </ul> <p></p> <ul> <li>Again check the two checkboxes at the top of each table definition to select all the columns for the join. Also, define the join columns by mapping the <code>CustomerID</code> from the first table to <code>CustomerID</code> on the second table. Do this by clicking on one and dragging it to another. When the line is drawn, click on the <code>Next</code> button.</li> </ul> <p></p> <p>Note: The blue line between the joined columns may disappear. The 'Join keys' view on the right-hand side should still contain the join definition, even if the line disappears.</p> <ul> <li> <p>In the next panel, although we could change the names of our columns, we will accept the existing names for our columns. Click the <code>Next</code> button to continue.</p> </li> <li> <p>In the next panel we'll give our joined data view a unique name (to be consistent with SQL standards, pick an all uppercase name), choose someething like: <code>XXXAPPLICANTFINANCIALPERSONALLOANSDATA</code> (where <code>XXX</code> is your initials in all upper case). Click the <code>Create view</code> button to add the virtualized aggregate view to your data.</p> </li> <li> <p>A pop up dialog panel will indicate that the join view creation has succeeded! Click on <code>Go to virtualized data</code> button.</p> </li> <li> <p>From the <code>Virtualized data</code> page you should now see all three virtualized tables and two joined tables. Do not go to the next section until you have all the tables.</p> </li> </ul> <p></p>"},{"location":"data-connection-and-virtualization/#conclusion","title":"Conclusion","text":"<p>In this section we learned how to make connection to databases that contain our data, how to virtualize them, and how to use the virtualized data.</p> <p>Remember that you can add data from different databases and servers if you need to. Moreover, you can virtualize the data from different sources together as well! The goal is to take care of bringing the data to the platform early on so all the data scientists can use that data while you keep full control of who has access to what data.</p>"},{"location":"data-grant-access/","title":"Grant Access to Virtualized Data","text":"<p>This section will continue exploring aspects of collecting data on Cloud Pak for Data. Specifically, there will be scenarios where a platform user (i.e a Data Engineer) will want to make data available to other users of the platform. This can be for any number of reasons. For this workshop, we will assume the scenario that one set of users know/understand the underlying data sources and will be responsible for creating virtualized views of the data needed. While a broader set of users will need to access the data (to assess, refine, etc). In this lab, we will make the data (views/tables/joins) that was virtualized in the <code>Data Virtualization</code> module, available to other users. In order for other users to have access to the data, you need to grant them access.</p> <p>Note: This section only needs to be completed if there are non-Admin or non-Data Engineer users you are working in a group with. The instructors would have indicated that it needs to be completed to give those users access to the data you have virtualized above.</p>"},{"location":"data-grant-access/#grant-access-to-the-virtualized-data","title":"Grant access to the virtualized data","text":"<ul> <li>To launch the data virtualization tool, go the (\u2630) navigation menu and under the <code>Data</code> section click on <code>Data virtualization</code>.</li> </ul> <ul> <li>From the Data virtualization sub-menu at the top left of the page, click on the menu drop down list and choose <code>My virtualized data</code>.</li> </ul> <ul> <li>For one of the virtualized data assets you've created, click the 3 vertical dots on the right and choose <code>Manage access</code>.</li> </ul> <ul> <li>Click the <code>Specific users</code> button and click the <code>Add user</code> button.</li> </ul> <ul> <li>In the popup dialog window, click the checkbox next to the user (or multiple users) you wish to grant access to and then click the <code>Add users</code> button.</li> </ul> <ul> <li>Repeat the above steps to give access to the remaining virtualized tables and views (all five that you created).</li> </ul>"},{"location":"data-grant-access/#conclusion","title":"Conclusion","text":"<p>In this section we learned how to allow users to collaborate and make use of virtualized data, without needing to go through the virtualization process themselves.</p>"},{"location":"data-visualization-and-refinery/","title":"Data Visualization and Data Refinery","text":"<p>Let's take a quick detour to the Data Refinery tool. Data Refinery can quickly filter and mutate data, create quick  visualizations, and do other data cleansing tasks from an easy-to-use user interface.</p> <p>Note: The lab instructions below assume you have a project already and have data you will refine. If not, follow the instructions in the pre-work and import data to project sections to create a project and assign data to your project.</p>"},{"location":"data-visualization-and-refinery/#1-load-data","title":"1. Load Data","text":"<ul> <li>Go the (\u2630) navigation menu and under the Projects section click on <code>All Projects</code>.</li> </ul> <ul> <li>Click the project name you created in the pre-work section to open it.</li> </ul> <ul> <li> <p>From the <code>Project</code> home, under the <code>Assets</code> tab, ensure the <code>Data assets</code> section is expanded or click on the arrow to toggle it and open up the list of data assets.</p> </li> <li> <p>Click the merged data asset  <code>XXXAPPLICANTFINANCIALPERSONALLOANDATA</code> (the name of the file may vary, <code>XXX</code> may be your initials or the initials of the person who granted you data access) to open it. If this is the first time you are opening the data asset, you will be asked to unlock the connection. Select 'Username/password' from the dropdown, click the <code>Use your Cloud Pak for Data credentials to authenticate to the data source</code> checkbox and then click the <code>Connect</code> button.</p> </li> <li> <p>Once the preview of the data asset opens, click on the <code>Prepare data</code> button on the top right of the table.</p> </li> </ul> <p></p> <ul> <li>Data Refinery will launch and open to the <code>Data</code> tab. It will also display the information panel with details of the data refinery flow and where the output of the flow will be placed. Click the <code>X</code> to the right of the <code>About this asset</code> panel to close it.</li> </ul> <p></p>"},{"location":"data-visualization-and-refinery/#2-refine-data","title":"2. Refine Data","text":"<p>We'll start out in the <code>Data</code> tab where we wrangle, shape and refine our data. As you refine your data, IBM Data Refinery keeps track of the steps in your data flow. You can modify them and even select a step to return to a particular moment in your data\u2019s transformation.</p>"},{"location":"data-visualization-and-refinery/#create-transformation-flow","title":"Create Transformation Flow","text":"<ul> <li>With Data Refinery, we can transform our data by directly entering operations in R syntax or interactively by selecting operations from the menu. For example, start typing <code>filter</code> on the Command line and observe that the list of operations displayed will get updated. Click on the filter operation.</li> </ul> <ul> <li>A <code>filter</code> operation syntax will be displayed in the Command line. Clicking on each element of the operation will give hints on the syntax and how to use the command. For instance, to filter for customers who have paid credits up to date, build the expression shown below. To enact the filter, you would <code>Apply</code> the expression.</li> </ul> <pre><code>filter(`CreditHistory` == 'credits_paid_to_date')\n</code></pre> <ul> <li>We can remove this custom filter by clicking on 'Delete' from the drop-down menu on the <code>Custom code</code> step of our data workflow.</li> </ul> <ul> <li>We can remove records with empty values in a particular column. Scroll over to the <code>StreetAddress</code> column, click on the three dots to open the action menu, and select <code>Remove empty rows</code>. You will see an entry added to the <code>Steps</code> view.</li> </ul> <ul> <li> <p>Let's say we've decided that there are columns that we don't want to leave in our dataset ( maybe because they might not be useful features in our Machine Learning model or because we don't want to make those data attributes accessible to others). We'll remove the <code>FirstName</code>, <code>LastName</code>, <code>Email</code>, <code>StreetAddress</code>, <code>City</code>, <code>State</code>, <code>PostalCode</code> columns.</p> </li> <li> <p>For each column to be removed: Click the <code>New step</code> button at the bottom of the page. Find the column in the table, click on the three dots to see the action menu, then select <code>Remove column</code>.</p> </li> </ul> <p></p> <p></p> <p></p> <ul> <li>At this point, you have a data transformation flow with 8 steps. As we saw in the last section, we keep track of each of the steps and we can even undo (or redo) an action using the circular arrows. To see the steps in the data flow that you have performed, click the <code>Steps</code> button. The operations that you have performed on the data will be shown.</li> </ul> <p></p>"},{"location":"data-visualization-and-refinery/#schedule-jobs","title":"Schedule Jobs","text":"<p>Data Refinery allows you to run jobs at scheduled times, and save the output. In this way, you can regularly refine new data as it is updated.</p> <ul> <li>Click on the \"jobs\" icon and then <code>Save and create job</code> option from the menu.</li> </ul> <p></p> <ul> <li>Give the job a name and optional description, then click the <code>Next</code> button.</li> </ul> <p></p> <ul> <li>The job will configure a default input and output data asset, as well as the runtime environment. Click the <code>Next</code> button.</li> </ul> <p></p> <ul> <li>We can set the job to run on a schedule. For now, leave the schedule off and click the <code>Next</code> button.</li> </ul> <p></p> <ul> <li>We can get notifications from job runs. For now, skip the notification configuration and click the <code>Next</code> button.</li> </ul> <p></p> <ul> <li>Click the <code>Create and Run</code> button to save and run this job.</li> </ul> <p></p> <ul> <li>This refinery flow will be saved to your project in the <code>Data Refinery flows</code> section of the project overview page. From that section you could revisit the flow to edit the steps or even see any execution jobs you have run. For now, we will move on to exploring our data.</li> </ul>"},{"location":"data-visualization-and-refinery/#3-profile-data","title":"3. Profile Data","text":"<ul> <li>Back on the top level of the data refinery view, click on the <code>Profile</code> tab to bring up a view of several statistics and histograms for the attributes in your data.</li> </ul> <ul> <li> <p>Once the data profile loads, you can get insight into the data from the views and statistics:</p> </li> <li> <p>The median age of the applicants is 36, with the bulk under 49.</p> </li> <li> <p>About as many people had credits_paid_to_date as prior_payments_delayed. Few had no_credits.</p> </li> <li> <p>The median was 3 years for duration at current residence. Range was 1-6 years.</p> </li> </ul>"},{"location":"data-visualization-and-refinery/#4-visualize-data","title":"4. Visualize Data","text":"<p>Let's do some visual exploration of our data using charts and graphs. Note that this is an exploratory phase and we're looking for insights in out data. We can accomplish this in Data Refinery interactively without coding.</p> <ul> <li>Choose the <code>Visualizations</code> tab to bring up the page where you can select columns that you want to visualize. Select <code>LoanAmount</code> from the \"Columns to visualize\" drop down list as the first column and click <code>Add another column</code> to add another column. Next add <code>LoanDuration</code> and click the <code>Visualize data</code> button. The system will pick a suggested plot for you based on your data and show more suggested plot types at the top.</li> </ul> <p></p> <ul> <li>Remember that we are most interested in knowing how these features impact a loan being at the risk. So, let's add the <code>Risk</code> as a color on top of our current scatter plot. That should help us visually see if there's something of interest here. From the left panel, click the <code>Color Map</code> drop down and select <code>Risk</code>. Also, to see the full data, drag the right side of the data selector at the bottom all the way to the right, in order to show all the data inside your plot.</li> </ul> <p></p> <ul> <li>We notice that there are more blue (risk) on this plot towards the top right, than there is on the bottom left. This is a good start as it shows that there is probably a relationship between the riskiness of a loan and its duration and amount. It appears that the higher the amount and duration, the riskier the loan. Interesting, let's dig in further in how the loan duration could play into the riskiness of a loan.</li> </ul> <p>Note: The colors used in your visualization may be different. Be sure to look at chart legend for clarification</p> <ul> <li> <p>Let's plot a histogram of the <code>LoanDuration</code> to see if we can notice anything. First, select <code>Histogram</code> from the <code>Chart Type</code>.</p> </li> <li> <p>On the left, select <code>LoanDuration</code> for the 'X-axis', select <code>Risk</code> in the 'Split By' section, check the <code>Stacked</code> option, uncheck the <code>Show kde curve</code> toggle, uncheck the <code>Show distribution curve</code> toggle. You should see a chart that looks like the following image.</p> </li> </ul> <p></p> <ul> <li> <p>It looks like the longer the duration the larger the blue bar (risky loan count) become and the smaller the dark blue bars (non risky loan count) become. That indicate loans with longer duration are in general more likely to be risky. However, we need more information.</p> </li> <li> <p>We next explore if there is some insight in terms of the riskiness of a loan based on its duration when broken down by the loan purpose. To do so, let's create a Heat Map plot.</p> </li> <li> <p>At the top of the page, in the <code>Chart Type</code> section, open the arrows on the right, select <code>Heat Map</code>.</p> </li> </ul> <p></p> <ul> <li>Next, select <code>Risk</code> in the column section and <code>LoanPurpose</code> for the <code>Row</code> section. Additionally, to see the effects of the loan duration, select <code>Mean</code> in the summary section, and select <code>LoanDuration</code> in the <code>Value</code> section.</li> </ul> <p></p> <ul> <li> <p>You can now see that the least risky loans are those taken out for purchasing a new car and they are on average 10 years long. To the left of that cell we see that loans taken out for the same purpose that average around 15 years for term length seem to be more risky. So one could conclude the longer the loan term is, the more likely it will be risky. In contrast, we can see that both risky and non-risky loans for the other category seem to have the same average term length, so one could conclude that there's little, if any, relationship between loan length and its riskiness for the loans of type other.</p> </li> <li> <p>In general, for each row, the bigger the color difference between the right and left column, the more likely that loan duration plays a role for the riskiness of the loan category.</p> </li> <li> <p>Now let's look into customizing our plot. Under the Actions panel, notice that you can perform tasks such as <code>Start over</code>, <code>Download chart details</code>, <code>Download chart image</code>, or set <code>Global visualization preferences</code> (Note: Hover over the icons to see the names). Click on the drop down arrow next to <code>Action</code>. Then click on the <code>Global visualization preferences</code> option from the menu.</p> </li> </ul> <p></p> <ul> <li>We see that we can do things in the <code>Global visualization preferences</code> for <code>Titles</code>, <code>Tools</code>, <code>Theme</code>, and <code>Notifications</code>. Click on the <code>Theme</code> tab and update the color scheme to <code>Dark</code>. Then click the <code>Apply</code> button, now the colors for all of our charts will reflect this. Play around with various Themes and find one that you like.</li> </ul> <p></p>"},{"location":"data-visualization-and-refinery/#conclusion","title":"Conclusion","text":"<p>We've seen a some of the capabilities of the Data Refinery. We saw how we can transform data using R code, as well as using various operations on the columns such as changing the data type, removing empty rows, or deleting the column altogether. We next saw that all the steps in our Data Flow are recorded, so we can remove steps, repeat them, or edit an individual step. We were able to quickly profile the data, to see histograms and statistics for each column. And finally we created more in-depth Visualizations, creating a scatter plot, histogram, and heatmap to explore the relationship between the riskiness of a loan and its duration, and purpose.</p>"},{"location":"faq/","title":"Frequently Asked Questions &amp; Helpful Tips / Tricks","text":"<ol> <li>I dont have the Jupyter notebooks in my project. How can I import them ?</li> <li>How can I see what the Jupyter notebooks output should be?</li> </ol>"},{"location":"faq/#manually-importing-jupyter-notebooks","title":"Manually Importing Jupyter Notebooks","text":"<p>During the pre-work section of this workshop, you create a project based on an existing project file. If, for some reason, you are not using the project zip file to create your project then you will not have all the assets (Jupyter Notebooks, CSV files, etc) necessary for the labs. You can manually import these assets into an existing or empty project. Use the following steps to import the Jupyter notebook files manually.</p> <ul> <li>At your project overview page, click the <code>Add to project</code> button, and choose the <code>Notebook</code> option.</li> </ul> <p></p> <ul> <li>On the next panel: select the <code>From URL</code> tab, give your notebook a name, provide the notebook URL, and leave the default Python environment:</li> </ul> <p>Note: The URL and name shown in the screenshot may not match your scenario. Use the table below to determine what URL to use.</p> <p></p> <ul> <li>Use the following table to find the URL to use for each of the Workshop notebooks:</li> </ul> Module Suggested Name URL ML Spark Model machinelearning-creditrisk-sparkmlmodel <code>https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/machinelearning-creditrisk-sparkmlmodel.ipynb</code> ML Model Deployment machinelearning-creditrisk-batchscoring <code>https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/machinelearning-creditrisk-batchscoring.ipynb</code> OpenScale Full Config openscale-full-configuration <code>https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-full-configuration.ipynb</code> OpenScale Manual Config - Initial Setup openscale-initial-setup <code>https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-initial-setup.ipynb</code> OpenScale Manual Config - Fairness / Explainability openscale-fairness-explainability <code>https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-fairness-explainability.ipynb</code> OpenScale Manual Config - Quality openscale-quality-feedback <code>https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-quality-feedback.ipynb</code> OpenScale Manual Config - Drift openscale-drift-config <code>https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-drift-config.ipynb</code> OpenScale Manual Config - Historic Data openscale-historic-data <code>https://raw.githubusercontent.com/IBM/credit-risk-workshop-cpd/master/notebooks/openscale-historic-data.ipynb</code> <ul> <li>Click the <code>Create notebook</code> button. The Jupyter notebook will be loaded and the kernel is started. Once the kernel is ready you can go back to the workshop instructions.</li> </ul>"},{"location":"faq/#jupyter-notebooks-with-output","title":"Jupyter Notebooks with Output","text":"<p>The Jupyter notebooks in the project file you import have been cleared of output. For reference, you can see the notebooks with output at the links listed below. Note: These links are not meant to be used for importing the notebook into the project.</p> Module Notebook Name Notebook With Output URL ML Spark Model machinelearning-creditrisk-sparkmlmodel <code>https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/machinelearning-creditrisk-sparkmlmodel-with-output.ipynb</code> ML Model Deployment machinelearning-creditrisk-batchscoring <code>https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/machinelearning-creditrisk-batchscoring-with-output.ipynb</code> OpenScale Full Config openscale-full-configuration <code>https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-full-configuration-output.ipynb</code> OpenScale Manual Config - Initial Setup openscale-initial-setup <code>https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-initial-setup-with-output.ipynb</code> OpenScale Manual Config - Fairness / Explainability openscale-fairness-explainability <code>https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-fairness-explainability-with-output.ipynb</code> OpenScale Manual Config - Quality openscale-quality-feedback <code>https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-quality-feedback-with-output.ipynb</code> OpenScale Manual Config - Drift openscale-drift-config <code>https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-drift-config-with-output.ipynb</code> OpenScale Manual Config - Historic Data openscale-historic-data <code>https://github.com/IBM/credit-risk-workshop-cpd/blob/master/notebooks/with-output/openscale-historic-data-with-output.ipynb</code>"},{"location":"gitIntegration/","title":"Git Repository Integration","text":"<p>IMPORTANT NOTE: This module has been archived and is no longer being actively maintained. The readme will be left below for reference only.</p> <p>A Cloud Pak for Data project can be integrated with a git repository. The Git integration must be done at project creation time.</p>"},{"location":"gitIntegration/#generate-gitlab-token","title":"Generate GitLab Token","text":"<ul> <li>To create a token for Gitlab, login to GitLab, click on your user account in the top right and choose <code>Settings</code>:</li> </ul> <ul> <li>From the left navigation bar select <code>Access tokens</code> and fill in the name, expiration date, and check the boxes for read_repository and write_repository. Finally, click <code>Create personal access token</code> button:</li> </ul> <ul> <li>On the resulting page, you'll see your personal access token. Copy this.</li> </ul> <p>NOTE: This token gives access to your git repository. Do not share with anyone.</p> <p></p>"},{"location":"gitIntegration/#create-analytics-project-with-git-integration","title":"Create Analytics Project with Git Integration","text":"<ul> <li>Go the (\u2630) navigation menu and click on the Projects link.</li> </ul> <ul> <li>Click on the New project button on the top right.</li> </ul> <ul> <li>Select the <code>Analytics project</code> radio button and click the Next button.</li> </ul> <ul> <li>We are going to create an empty project. Select the Create an empty project option.</li> </ul> <ul> <li>Give the project a name and Click the box for <code>Integrate this project with git</code>. Then click the <code>New Token</code> link on the right.</li> </ul> <ul> <li>In the Git Integration panel: select the Gitlab option from the Platform drop down list, Paste in your access token from the previous section, Enter your GitLab username, provide a name to the token. Then click the <code>Continue</code> button.</li> </ul> <ul> <li>Now select the token you just created in the Token drop down list. Put in the repository URL, Select a branch, and click <code>Create</code>.</li> </ul> <p>Note: We are assuming you have a project already created in the repository that you will synch to. If not, go ahead and create an empty project in GitLab</p>"},{"location":"gitIntegration/#create-asset-and-push-to-gitlab","title":"Create Asset and Push to GitLab","text":"<p>Now whenever we create an asset in the project, we will be able to push it to the GitLab repository.</p> <ul> <li>From the project overview, either click the <code>Add to project +</code> button, and choose <code>Notebook</code>.</li> </ul> <p></p> <ul> <li>On the 'New notebook' panel, give the notebook a name and optional description. Leave the default runtime. Click the <code>Create notebook</code> button.</li> </ul> <p></p> <ul> <li>We can start making changes to the notebook but first lets sync with the repository. Go back to the top-level project page by clicking on the project name in the navigation hierarchy in the top left of the page.</li> </ul> <p></p> <ul> <li>Click the \"circular arrow\" sync-icon and choose the <code>Pull and Push</code> option from the menu.</li> </ul> <p></p> <ul> <li>On the Confirm Sync page, select your token, check the box next to your notebook, and click the <code>Sync</code> button.</li> </ul> <p></p> <ul> <li> <p>A window will come up asking you to verify \"Did you remove credentials from assets?\". After confirming this, click the <code>Continue export</code> button.</p> </li> <li> <p>After syncing, the window will show Success. You can click the <code>Back to project</code> button.</p> </li> <li> <p>You can check your GitLab project to see the notebook has been added under the 'assets' directory.</p> </li> </ul>"},{"location":"gitIntegration/#conclusion","title":"Conclusion","text":"<p>In this section we covered how to enable the Git integration to your projects in Cloud Pak for Data. You can integrate git into your workflow in your usual way, syncing with teammates via 'git pull' and using 'git push' to upload your changes to the git remote repository.</p>"},{"location":"machine-learning-autoai/","title":"Automate model building with AutoAI","text":"<p>For this part of the workshop, we'll learn how to use AutoAI. AutoAI is a capability that automates various tasks to ease the workflow for data scientists that are creating machine learning models. It automates steps such as preparing your data for modeling, chooses the best algorithm/estimator for your problem, experiments with pipelines and parameters for the trained models.</p> <p>Note: The lab instructions below assume you have a project already with the assets necessary to build a model. If not, follow the instructions in the pre-work section to create a project.</p>"},{"location":"machine-learning-autoai/#1-run-autoai-experiment","title":"1. Run AutoAI Experiment","text":"<ul> <li>Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project.</li> </ul> <ul> <li>To start the AutoAI experiment, click the <code>New asset</code> button from the top of the page.</li> </ul> <ul> <li>From the dialog, select select the <code>AutoAI experiment</code> option.</li> </ul> <ul> <li>Name your AutoAI experiment asset and leave the default compute configuration option listed in the drop-down menu. Then click the <code>Create</code> button.</li> </ul> <ul> <li>To configure the experiment, we must first give it the dataset that will be used to train the machine learning model. We will be using one of the CSV file datasets we have preloaded into the project. Click on the <code>Select data from project</code> option.</li> </ul> <ul> <li>In the dialog, select the <code>german_credit_data.csv</code> file and click the <code>Select asset</code> button.</li> </ul> <ul> <li> <p>Once the dataset is read in, we will need to configure the details of the AutoAI experiment. Next, indicate what we want the model to predict. Under \"Configure details\", select <code>No</code> for \"Create a time series analysis?\". In the What do you want to predict? dropdown, find and click on the <code>Risk</code> row.</p> </li> <li> <p>AutoAI will set up defaults values for the experiment based on the dataset and the column selected for the prediction. This includes the type of model to build, the metrics to optimize against, the test/train split, etc. To view/change these values, click the <code>Experiment settings</code> button.</p> </li> </ul> <p></p> <ul> <li>On the <code>Data source settings</code> panel, in the <code>Select features to include</code> section, deselect the checkbox for the <code>CustomerID</code> column name. This will remove the customer ID column from being used as a feature for the model. Although we could change other aspects of the experiment, we will accept the remaining default values and click the <code>Save settings</code> button.</li> </ul> <p></p> <ul> <li>To start the experiment, click on the <code>Run experiment</code> button.</li> </ul> <p></p> <ul> <li> <p>The AutoAI experiment will now run. AutoAI will run through steps to prepare the dataset, split the dataset into training / evaluation groups and then find the best performing algorithms / estimators for the type of model. It will then build the following series of candidate pipelines for each of the top N performing algorithms (where N is a number chosen in the configuration which defaults to 2):</p> <ul> <li>Baseline model (Pipeline 1)</li> <li>Hyperparameter optimization (Pipeline 2)</li> <li>Automated feature engineering (Pipeline 3)</li> <li>Hyperparameter optimization on top of engineered features(Pipeline 4)</li> </ul> </li> <li> <p>The UI will show progress as different algorithms/evaluators are selected and as different pipelines are created &amp; evaluated. You can view the performance of the pipelines that have completed by expanding each pipeline section in the leaderboard.</p> </li> </ul> <p></p> <ul> <li>The experiment can take several minutes to run. Upon completion you will see a message that the pipelines have been created. Do not proceed to the next section until the experiment completes.</li> </ul>"},{"location":"machine-learning-autoai/#2-save-autoai-model","title":"2. Save AutoAI Model","text":"<ul> <li>Once the experiment completes, you can explore the various pipelines and options in the UI. Some of the options available are to see a comparison of the pipelines, to change the ranking based on a different performance metric, to see a log of the experiment, or to see the ranked listing of the pipelines (ranking based on the optimization metric in your experiment, in this case accuracy.)</li> </ul> <ul> <li> <p>Scroll down to see the Pipeline leaderboard. The top performing pipeline is in the first rank.</p> </li> <li> <p>The next step is to select the model that gives the best result and view its performance. In this case, Pipeline 5 gave the best result for our experiment. You can view the detailed results by clicking the corresponding pipeline name from the leaderboard:</p> </li> </ul> <p></p> <ul> <li>The model evaluation page will show metrics for the experiment, confusion matrix, feature transformations that were performed (if any), which features contribute to the model, and more details of the pipeline. Optionally, feel free to click through these views of the pipeline details. In order to deploy this model, click on the <code>Save as</code> button.</li> </ul> <p></p> <p>On the next screen, select the <code>Model</code> option. You can optionally change the name, add an optional description or tags, then click <code>Create</code> to save it.</p> <p></p> <ul> <li>You receive a notification to indicate that your model is saved to your project. You can return to your project using the notification by clicking <code>View in project</code>, or go back to your project main page by clicking on the project name on the navigator on the top left.</li> </ul> <p></p> <ul> <li>You will see the new model under Models section of the Assets page:</li> </ul>"},{"location":"machine-learning-autoai/#3-save-autoai-notebook","title":"3. Save AutoAI notebook","text":"<ul> <li>To save the AutoAI experiment as a notebook, go back to the window for the pipeline you have chosen, and click <code>Save as</code>.</li> </ul> <p>Note: You can get back to the pipeline window by going to your project overview page, clicking on the AutoAI experiment and clicking the pipeline from the leaderboard</p> <p></p> <ul> <li>Choose the <code>Notebook</code> tile, accept the default name or change it if you like. Add optional description or tags, and click <code>Create</code>.</li> </ul> <p></p> <ul> <li>You will receive a notification to indicate that your notebook is saved to your project. Close the pipeline details window to expose the path back to the project at the top of the screen. Click on your project name to navigate to the project overview page.</li> </ul> <p></p> <ul> <li>The notebook will be saved to your project, and can be examined in detail, changed and modified, and used to create a new model. See the documentations for Modifying and running an AutoAI generated notebook for details.</li> </ul>"},{"location":"machine-learning-autoai/#4-promote-the-model","title":"4. Promote the model","text":"<ul> <li>Now that we have saved our model, we will next need to make the model available in our deployment space so it can be deployed. Under the Models section of the Assets page, click the name of your saved model.</li> </ul> <ul> <li>To make the model available to be deployed, we need to promote it to a deployment space we previously set up. Click on the <code>Promote to deployment space</code>:</li> </ul> <p>***Note: This is assuming you have already created a deployment space in the pre-work section of the workshop. </p> <ul> <li>Add an optional description or tags if you'd like. Click on the <code>Promote</code> button.</li> </ul> <p></p> <ul> <li>You will see a notification that the model was promoted to the deployment space succesfully.</li> </ul> <p></p>"},{"location":"machine-learning-autoai/#conclusion","title":"Conclusion","text":"<p>In this section we covered one approach to building machine learning models on Cloud Pak for Data. We have seen how AutoAI helps find an optimal model by automating tasks such as:</p> <ul> <li>Data Wrangling</li> <li>Model Algorithm Evaluation &amp; Selection</li> <li>Feature Engineering</li> <li>Hyperparameter Optimization.</li> </ul>"},{"location":"machine-learning-autoai/running-autoai-notebook/","title":"Modifying and Running an AutoAI generated notebook","text":"<p>Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do.</p> <p>You will run cells individually by highlighting each cell, then either click the <code>Run</code> button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk ([*]) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. [17]).</p> <p></p> <p>The notebook generated is pre-filled with Python code. Most of the sections will be configured with the appropriate  values or with the logic required to lookup the necessary values. The one section that requires an update is Create online deployment.</p>"},{"location":"machine-learning-autoai/running-autoai-notebook/#create-online-deployment","title":"Create online deployment","text":"<p>In the Working with spaces subsection, update the <code>space_id</code> value with the name of the deployment space that you created previously.</p>"},{"location":"machine-learning-autoai/running-autoai-notebook/#conclusion","title":"Conclusion","text":"<p>In this part of the lab, we examined and ran a Jupyter notebook that was generated as the result of an AutoAI experiment. Feel free to modify and re-run the notebook, making any changes that you are comfortable with.</p>"},{"location":"machine-learning-deployment-scoring/","title":"Machine Learning Model Deployment and Scoring","text":"<p>In this module, we will go through the process of deploying a machine learning model so it can be used by others and its performance can be monitored and validated. The deployment will result in an endpoint that makes the model available for wider use in applications and to make business decisions. There are several types of deployments available (depending on the model framework used), of which we will explore:</p> <ul> <li>Online Deployments - Creates an endpoint to generate a score or prediction in real time.</li> <li>Batch Deployments - Creates an endpoint to schedule the processing of bulk data to return predictions.</li> </ul> <p>This module is broken up into several sections that explore the different model deployment options as well as the different ways to invoke or consume the model. The first section of this lab will build an online deployment and test the model endpoint using both the built-in testing tool as well as external testing tools. The remaining sections are optional, they build and test the batch deployment, followed by using the model from a python application.</p> <p>Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the <code>Workshop Resources</code> -&gt; <code>FAQs / Tips</code> section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the <code>Workshop Resources</code> -&gt; <code>FAQs / Tips</code> section for links to the completed notebooks.</p> <p>Note: It is also assumed that you have completed one of the machine learning modules to promote a model to the deployment space. If not, follow the instructions in one of the machine learning modules to create and promote a machine learning model.</p>"},{"location":"machine-learning-deployment-scoring/#online-model-deployment","title":"Online Model Deployment","text":""},{"location":"machine-learning-deployment-scoring/#deploy-online-model","title":"Deploy Online Model","text":"<p>After a model has been created and saved / promoted to our deployment space, we will want to deploy the model so it can be used by others. For this section, we will be creating an online deployment. This type of deployment will make an instance of the model available to make predictions in real time via an API. Although we will use the Cloud Pak for Data UI to deploy the model, the same can be done programmatically.</p> <ol> <li> <p>Navigate to the left-hand (\u2630) hamburger menu and click on <code>Deployments</code>.</p> <p></p> </li> <li> <p>Click on the <code>Spaces</code> tab and then choose the deployment space you setup previously by clicking on the name of your space.</p> <p></p> </li> <li> <p>From your deployment space overview, select the <code>Assets</code> tab and find the model for which you want to create a deployment. Click on the \"kebab\" menu (three vertical dots) on the right side of the model row and select <code>Deploy</code>.</p> <p>Note: There may be more than one model listed in the 'Models' section. This can happen if you have run the Jupyter notebook more than once or if you have run through both the Jupyter notebook and AutoAI modules to create models. Although you could select any of the models you see listed in the page, the recommendation is to start with whichever model is available that is using a <code>spark-mllib_X.X</code> software specification.</p> <p></p> </li> <li> <p>On the 'Create a deployment' screen, choose <code>Online</code> for the <code>Deployment Type</code>, give the Deployment a name and optional description and click the <code>Create</code> button.</p> <p></p> </li> <li> <p>Click on the <code>Deployments</code> tab. The online deployment will show as <code>Initializing</code> and then switch to <code>Deployed</code> when done.</p> <p></p> </li> </ol>"},{"location":"machine-learning-deployment-scoring/#test-online-model-deployment","title":"Test Online Model Deployment","text":"<p>The platform offers tools to quickly test out Watson Machine Learning models. We begin with the built-in tooling.</p> <ol> <li> <p>From the Model deployment page, once the deployment status shows as <code>Deployed</code>, click on the name of your deployment.</p> <p></p> </li> <li> <p>The deployment <code>API reference</code> tab shows how to use the model using <code>cURL</code>, <code>Java</code>, <code>Javascript</code>, <code>Python</code>, and <code>Scala</code>.</p> </li> </ol> <p></p> <ol> <li> <p>To get to the built-in test tool, click on the <code>Test</code> tab and then click on the <code>JSON input</code> tab.</p> <p></p> </li> <li> <p>Copy and paste the following data objects into the <code>Body</code> panel (replace the text that was in the input panel).</p> <p>Note: Click the tab appropriate for the model you are testing (either an AutoAI model or one built using the Jupyter notebook). Also make sure the input below is the only content in the field. Do not append it to the default content <code>{ \"input_data\": [] }</code> that may already be in the test input panel.</p> Jupyter Spark Model <pre><code>{ \"input_data\": [{\n\"fields\": [ \"CheckingStatus\", \"LoanDuration\", \"CreditHistory\", \"LoanPurpose\", \"LoanAmount\", \"ExistingSavings\", \"EmploymentDuration\", \"InstallmentPercent\", \"Sex\", \"OthersOnLoan\", \"CurrentResidenceDuration\", \"OwnsProperty\", \"Age\", \"InstallmentPlans\", \"Housing\", \"ExistingCreditsCount\", \"Job\", \"Dependents\", \"Telephone\", \"ForeignWorker\"],\n\"values\": [[ \"no_checking\", 13, \"credits_paid_to_date\", \"car_new\", 1343, \"100_to_500\", \"1_to_4\", 2, \"female\", \"none\", 3, \"savings_insurance\", 46, \"none\", \"own\", 2, \"skilled\", 1, \"none\", \"yes\"]]\n}]}\n</code></pre> AutoAI Model <pre><code>{ \"input_data\": [{\n\"fields\": [ \"CustomerId\", \"CheckingStatus\", \"LoanDuration\", \"CreditHistory\", \"LoanPurpose\", \"LoanAmount\", \"ExistingSavings\", \"EmploymentDuration\", \"InstallmentPercent\", \"Sex\", \"OthersOnLoan\", \"CurrentResidenceDuration\", \"OwnsProperty\", \"Age\", \"InstallmentPlans\", \"Housing\", \"ExistingCreditsCount\", \"Job\", \"Dependents\", \"Telephone\", \"ForeignWorker\"],\n\"values\": [[ \"\", \"no_checking\", 13, \"credits_paid_to_date\", \"car_new\", 1343, \"100_to_500\", \"1_to_4\", 2, \"female\", \"none\", 3, \"savings_insurance\", 46, \"none\", \"own\", 2, \"skilled\", 1, \"none\", \"yes\"]]\n}]}\n</code></pre> </li> <li> <p>Click the <code>Predict</code> button.</p> <p></p> </li> <li> <p>The model will be called with the input data and the results will displayed in a dialog. Click on the <code>JSON view</code> radio button and scroll to the bottom to see the prediction (i.e \"Risk\" or \"No Risk\").</p> <p></p> </li> </ol>"},{"location":"machine-learning-deployment-scoring/#optional-test-online-model-deployment-using-curl","title":"(Optional) Test Online Model Deployment using cURL","text":"<p>Now that the model is deployed, we can also test it from external applications. One way to invoke the model API is using the cURL command.</p> <p>NOTE: Windows users will need the cURL command. It's recommended to download gitbash for this, as you'll also have other tools and you'll be able to easily use the shell environment variables in the following steps. Also note that if you are not using gitbash, you may need to change export commands to set commands.</p> <ol> <li>From the Model deployment page, once the deployment status shows as <code>Deployed</code>, click on the name of your deployment.</li> </ol> <p></p> <ol> <li>On deployment <code>API reference</code> copy the host name of the endpoint provided (the value before the slash (<code>/</code>). This will be the \"Environment Endpoint\".</li> </ol> <p></p> <ol> <li> <p>In a terminal window (or command prompt in Windows), run the following command to get a token to access the API. Replace <code>&lt;username&gt;</code> and <code>&lt;password&gt;</code> with the username and password you used to log into the Cloud Pak for Data environment. Replace <code>&lt;environment-endpoint&gt;</code> with the \"Environment Endpoint\" value from the previous step.</p> <pre><code>curl -k -X GET https://&lt;environment-endpoint&gt;/v1/preauth/validateAuth -u &lt;username&gt;:&lt;password&gt;\n</code></pre> </li> <li> <p>A json string will be returned with a value for \"accessToken\" that will look similar to this:</p> <pre><code>{\"username\":\"scottda\",\"role\":\"Admin\",\"permissions\":[\"access_catalog\",\"administrator\",\"manage_catalog\",\"can_provision\"],\"sub\":\"scottda\",\"iss\":\"KNOXSSO\",\"aud\":\"DSX\",\"uid\":\"1000331002\",\"authenticator\":\"default\",\"accessToken\":\"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VybmFtZSI6InNjb3R0ZGEiLCJyb2xlIjoiQWRtaW4iLCJwZXJtaXNzaW9ucyI6WyJhY2Nlc3NfY2F0YWxvZyIsImFkbWluaXN0cmF0b3IiLCJtYW5hZ2VfY2F0YWxvZyIsImNhbl9wcm92aXNpb24iXSwic3ViIjoic2NvdHRkYSIsImlzcyI6IktOT1hTU08iLCJhdWQiOiJEU1giLCJ1aWQiOiIxMDAwMzMxMDAyIiwiYXV0aGVudGljYXRvciI6ImRlZmF1bHQiLCJpYXQiOjE1NzM3NjM4NzYsImV4cCI6MTU3MzgwNzA3Nn0.vs90XYeKmLe0Efi5_3QV8F9UK1tjZmYIqmyCX575I7HY1QoH4DBhon2fa4cSzWLOM7OQ5Xm32hNUpxPH3xIi1PcxAntP9jBuM8Sue6JU4grTnphkmToSlN5jZvJOSa4RqqhjzgNKFoiqfl4D0t1X6uofwXgYmZESP3tla4f4dbhVz86RZ8ad1gS1_UNI-w8dfdmr-Q6e3UMDUaahh8JaAEiSZ_o1VTMdVPMWnRdD1_F0YnDPkdttwBFYcM9iSXHFt3gyJDCLLPdJkoyZFUa40iRB8Xf5-iA1sxGCkhK-NVHh-VTS2XmKAA0UYPGYXmouCTOUQHdGq2WXF7PkWQK0EA\",\"_messageCode_\":\"success\",\"message\":\"success\"}\n</code></pre> </li> <li> <p>You will save this access token to a temporary environment variable in your terminal. Copy the access token value (without the quotes) in the terminal and then use the following export command to save the \"accessToken\" to a variable called <code>WML_AUTH_TOKEN</code>.</p> <pre><code>export IAM_AUTH_TOKEN=&lt;value-of-access-token&gt;\n</code></pre> </li> <li> <p>Back on the model deployment page, gather the <code>URL</code> to invoke the deployed model from the API reference by copying the <code>Endpoint</code>.</p> <p></p> </li> <li> <p>Now save that endpoint to a variable named <code>URL</code> in your terminal by exporting it.</p> <pre><code>export URL=&lt;value-of-endpoint&gt;\n</code></pre> </li> <li> <p>Now run this curl command from the terminal to invoke the model with the same payload we used previously:</p> <pre><code>curl -k -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' --header \"Authorization: Bearer  $IAM_AUTH_TOKEN\" -d '{\"input_data\": [{\"fields\": [ \"CheckingStatus\", \"LoanDuration\", \"CreditHistory\", \"LoanPurpose\", \"LoanAmount\", \"ExistingSavings\", \"EmploymentDuration\", \"InstallmentPercent\", \"Sex\", \"OthersOnLoan\", \"CurrentResidenceDuration\", \"OwnsProperty\", \"Age\", \"InstallmentPlans\", \"Housing\", \"ExistingCreditsCount\", \"Job\", \"Dependents\", \"Telephone\", \"ForeignWorker\"],\"values\": [[ \"no_checking\", 13, \"credits_paid_to_date\", \"car_new\", 1343, \"100_to_500\", \"1_to_4\", 2, \"female\", \"none\", 3, \"savings_insurance\", 46, \"none\", \"own\", 2, \"skilled\", 1, \"none\", \"yes\"]]}]}' $URL\n</code></pre> </li> <li> <p>A json string will be returned with the response, including a prediction from the model (i.e a \"Risk\" or \"No Risk\" at the end indicating the prediction of this loan representing risk).</p> </li> </ol>"},{"location":"machine-learning-deployment-scoring/#optional-batch-model-deployment","title":"(Optional) Batch Model Deployment","text":"<p>Another approach to expose the model to be consumed by other users/applications is to create a batch deployment. This type of deployment will make an instance of the model available to make predictions against data assets or groups of records. The model prediction requests are scheduled as jobs, which are executed asynchronously.</p>"},{"location":"machine-learning-deployment-scoring/#deploy-batch-model","title":"Deploy Batch Model","text":"<ol> <li> <p>Navigate to the left-hand (\u2630) hamburger menu and click on <code>Deployments</code>.</p> <p></p> </li> <li> <p>Click on the <code>Spaces</code> tab and then choose the deployment space you setup previously by clicking on the name of your space.</p> <p></p> </li> <li> <p>From your deployment space overview, select the <code>Assets</code> tab and find the model for which you want to create a deployment. Click on the \"kebab\" menu (three vertical dots) on the right side of the model row and select <code>Deploy</code>.</p> </li> </ol> <p>Note: There may be more than one model listed in the 'Models' section. This can happen if you have run the Jupyter notebook more than once or if you have run through both the Jupyter notebook and AutoAI modules to create models. Although you could select any of the models you see listed in the page, the recommendation is to start with whichever model is available that is using a <code>spark-mllib_X.X</code> software specification.</p> <p></p> <ol> <li> <p>On the 'Create a deployment' screen: choose <code>Batch</code> for the <code>Deployment Type</code>, give the deployment a name and optional description. From the 'Hardware specification' drop down, select the smallest option (<code>1 standard CPU, 4GB RAM</code> in this case though for large or frequent batch jobs, you might choose to scale the hardware up). Click the <code>Create</code> button.</p> <p></p> </li> <li> <p>Once the status shows as <code>Deployed</code> you will be able to start submitting jobs to the deployment.</p> <p></p> </li> </ol>"},{"location":"machine-learning-deployment-scoring/#create-and-schedule-a-job","title":"Create and Schedule a Job","text":"<p>Next we can schedule a job to run against our batch deployment. We could create a job, with specific input data (or data asset) and schedule, either programmatically or through the UI. For this lab, we are going to do this programmatically using the Python client SDK. For this part of the exercise we're going to use a Jupyter notebook to create and submit a batch job to our model deployment.</p> <p>Note: The batch job input is impacted by the machine learning framework used to build the model. Currently, SparkML based model batch jobs require inline payload to be used. For other frameworks, we can use data assets (i.e CSV files) as the input payload.</p>"},{"location":"machine-learning-deployment-scoring/#open-the-batch-notebook","title":"Open the Batch Notebook","text":"<p>The Jupyter notebook is already included as an asset in the project you imported earlier.</p> <ol> <li> <p>Go the (\u2630) navigation menu and under the Projects section click on <code>All Projects</code>.</p> <p></p> </li> <li> <p>Click the project name you created in the pre-work section.</p> </li> <li> <p>From your <code>Project</code> overview page, click on the <code>Assets</code> tab to open the assets page where your project assets are stored and organized.</p> </li> <li> <p>Click on the <code>Notebooks</code> asset type, click on the \"kebob\" icon (vertical dots) on the right of the <code>machinelearning-creditrisk-batchscoring</code> notebook, and click <code>Edit</code>.</p> <p></p> </li> <li> <p>When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.</p> </li> </ol>"},{"location":"machine-learning-deployment-scoring/#run-notebook-sections","title":"Run Notebook sections","text":"<p>With the notebook open, spend a minute looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do. You will run cells individually by highlighting each cell, then either click the <code>Run</code> button at the top of the notebook or hitting the keyboard short-cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk (<code>[*]</code>) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. <code>[17]</code>).</p> <p>Please note that some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell.</p> <ol> <li> <p>Section <code>1.0 Install required packages</code> will install some of the libraries we are going to use in the notebook (many libraries come pre-installed on Cloud Pak for Data). Note that we upgrade the installed version of Watson Machine Learning Python Client. Ensure the output of the first code cell is that the python packages were successfully installed. (If the <code>ibm-watson-machine-learning</code> step fails, remove the <code>=1.0.53</code> from the line and re-run.)</p> <p></p> </li> <li> <p>Section <code>2.0 Create Batch Deployment Job</code> will create a job for the batch deployment. To do that, we will use the Watson Machine Learning client to get our deployment and create a job.</p> <ul> <li> <p>In the first code cell for <code>Section 2.1</code>, be sure to update the <code>wml_credentials</code> variable.</p> </li> <li> <p>The url should be the hostname of the Cloud Pak for Data instance.</p> </li> <li>The username and password should be the same credentials you used to log into Cloud Pak for Data.</li> <li> <p>Update the version number to <code>4.6</code>.</p> </li> <li> <p>In section 2.2, be sure to update the <code>DEPLOYMENT_SPACE_NAME</code> variable with your deployment space name (copy and paste the name which is within the output of the previous code cell).</p> </li> </ul> <p></p> <ul> <li>In section 2.3, be sure to update the <code>DEPLOYMENT_NAME</code> variable with the name of the batch deployment you created previously (copy and paste the name which is within the output of the previous code cell).</li> </ul> <p></p> <ul> <li>Continue to run the rest of the cells in section 2 which will load the batch input data set and create the job. The last code cell in section 2 will show that the job is in a queued state.</li> </ul> </li> <li> <p>Section <code>3.0 Monitor Batch Job Status</code> will start polling the job status until it completes or fails. The code cell will output the status every 5 seconds as the job goes from queued to running to completed or failed.</p> <p></p> </li> <li> <p>Once the job completes, continue to run the cells until the end of the notebook.</p> </li> </ol>"},{"location":"machine-learning-deployment-scoring/#cleanup-and-stop-environment","title":"Cleanup and Stop Environment","text":"<p>Important: In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done.</p> <ol> <li> <p>Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page.</p> <p></p> </li> <li> <p>Click on the <code>Manage</code> tab near the top of the page. Then in the <code>Environments</code> section, you will see the environment used by your notebook. Check the box next to the environment and select the <code>Stop runeimes</code> button at the top.</p> <p></p> </li> <li> <p>Click the <code>Stop</code> button on the subsequent pop up window.</p> </li> </ol>"},{"location":"machine-learning-deployment-scoring/#conclusion","title":"Conclusion","text":"<p>In this section we covered the followings:</p> <ul> <li>Creating and Testing Online Deployments for models.</li> <li>(Optional) Creating and Testing Batch Deployments for models.</li> <li>(Optional) Integrating the model deployment in an external application.</li> </ul> <p>Taking a predictive model and infusing AI into applications.</p>"},{"location":"machine-learning-in-jupyter-notebook/","title":"Machine Learning in Jupyter Notebook","text":"<p>In this module, we will go through the process of exploring our data set and building a predictive model that can be used to determine the likelyhood of a credit loan having 'Risk' or 'No Risk'. For this use case, the machine learning model we are building is a classification model that will return a prediction of 'Risk' (the features of the loan applicant predict that there is a good chance of default on the loan) or No Risk (the applicant's inputs predict that the loan will be paid off). The approach we will take in this lab is to some fairly popular libraries / frameworks to build the model in Python using a Jupyter notebook. Once we have built the model, we will make it available for deployment so that it can be used by others.</p> <p>Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the <code>Workshop Resources</code> -&gt; <code>FAQs / Tips</code> section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the <code>Workshop Resources</code> -&gt; <code>FAQs / Tips</code> section for links to the completed notebooks.</p>"},{"location":"machine-learning-in-jupyter-notebook/#build-and-save-a-model","title":"Build and Save a model","text":"<p>For this part of the exercise we're going to use a Jupyter notebook to create the model. The Jupyter notebook is already included as an asset in the project you imported earlier.</p>"},{"location":"machine-learning-in-jupyter-notebook/#open-the-jupyter-notebook","title":"Open the Jupyter notebook","text":"<ul> <li>Go the (\u2630) navigation menu and under the Projects section click on <code>All Projects</code>.</li> </ul> <ul> <li> <p>Click the project name you created in the pre-work section.</p> </li> <li> <p>From your <code>Project</code> overview page, click on the <code>Assets</code> tab to open the assets page where your project assets are stored and organized.</p> </li> <li> <p>Scroll down to the <code>Notebooks</code> section of the page and click on the \"kebab\" icon (three vertical dots) to right of the <code>machinelearning-creditrisk-sparkmlmodel</code> notebook then click <code>Edit</code>.</p> </li> </ul> <p></p> <ul> <li>When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.</li> </ul> <p></p>"},{"location":"machine-learning-in-jupyter-notebook/#run-the-jupyter-notebook","title":"Run the Jupyter notebook","text":"<p>Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do.</p> <p>You will run cells individually by highlighting each cell, then either click the <code>Run</code> button at the top of the notebook or hitting the keyboard short cut to run the cell (<code>Shift + Enter</code> but can vary based on platform). While the cell is running, an asterisk (<code>[*]</code>) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. <code>[17]</code>).</p> <p>Note: Some of the comments in the notebook (those in bold red) are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell.</p>"},{"location":"machine-learning-in-jupyter-notebook/#load-and-prepare-dataset","title":"Load and Prepare Dataset","text":"<ul> <li> <p>Section <code>1.0 Install required packages</code> will install some of the libraries we are going to use in the notebook (many libraries come pre-installed on Cloud Pak for Data). Note that we upgrade the installed version of Watson Machine Learning Python Client. Ensure the output of the first code cell is that the python packages were successfully installed.</p> <ul> <li>Run the code cells in section 1.1 and 1.2. Ensuring that the cells complete before continuing.</li> </ul> <p></p> </li> <li> <p>Section <code>2.0 Load and Clean data</code> will load the data set we will use to build out machine learning model. In order to import the data into the notebook, we are going to use the code generation capability of Watson Studio.</p> <ul> <li> <p>Highlight the code cell below by clicking it. Ensure you place the cursor below the first comment line.</p> </li> <li> <p>Click the <code>` \"Code Snippets\" icon in the upper right of the notebook to find the data asset from your project. Click on the</code>Read data` button.</p> </li> </ul> <p></p> <ul> <li>Click on the <code>Select data from project</code> button.</li> </ul> <p></p> <ul> <li> <p>Click on <code>Data asset</code> to see the data available within the project. </p> </li> <li> <p>If you are using virtualized data, then choose your virtualized merged view (i.e. <code>USERXXXX.APPLICANTFINANCIALPERSONALLOANSDATA</code>). If you are using this notebook without virtualized data, you can use the <code>german_credit_data.csv</code> CSV file version of the data set that has been included in the project. Click <code>Select</code>.</p> </li> </ul> <p></p> <ul> <li>For your dataset, select <code>pandas DataFrame</code> from the \"Load as\" dropdown. Click on <code>Insert code to cell</code> to add the snippet to the notebook.</li> </ul> <p></p> <ul> <li>Run the cell and you will see the first five rows of our dataset.</li> </ul> <p></p> </li> <li> <p>Since we are using generated code to import the data, you will need to update the next cell to assign the <code>df</code> variable. Copy the variable that was generated in the previous cell ( it will look like <code>df=data_df_1</code>, <code>data_df_2</code>, etc) and assign it to the <code>df</code> variable (for example <code>df=df_data_1</code>).</p> </li> </ul> <p></p> <ul> <li>Continue to run the remaining cells in section 2 to explore and clean the data.</li> </ul>"},{"location":"machine-learning-in-jupyter-notebook/#build-machine-learning-model","title":"Build Machine Learning Model","text":"<ul> <li> <p>Section <code>3.0 Create a model</code> cells will run through the steps to build a model pipeline.</p> <ul> <li>We will split our data into training and test data, encode the categorial string values, create a model using the Random Forest Classifier algorithm, and evaluate the model against the test set.</li> <li>Run all the cells in section 3 to build the model.</li> </ul> </li> </ul> <p></p>"},{"location":"machine-learning-in-jupyter-notebook/#save-the-model","title":"Save the model","text":"<ul> <li> <p>Section <code>4.0 Save the model</code> will save the model to your project.</p> </li> <li> <p>We will be saving and deploying the model to the Watson Machine Learning service within our Cloud Pak for Data platform. In the first code cell in section 4.1, be sure to update the <code>wml_credentials</code> variable as follows:</p> <ul> <li>The url should be the full hostname of the Cloud Pak for Data instance, which you can copy from your browsers address bar (for example, it may look like this: <code>https://zen.clustername.us-east.containers.appdomain.cloud</code>)</li> <li>The username and password should be the same credentials you used to log into Cloud Pak for Data.</li> </ul> </li> <li> <p>You will update the <code>MODEL_NAME</code> and <code>DEPLOYMENT_SPACE_NAME</code> variables. For the <code>MODEL_NAME</code>, create a unique and easily identifiable model name. For the <code>DEPLOYMENT_SPACE_NAME</code>, copy the name of your deployment space which was output in the previous code cell.</p> </li> </ul> <pre><code>MODEL_NAME = \"user123 credit risk model\"\nDEPLOYMENT_SPACE_NAME = \"Name you used for deployment space\"\n</code></pre> <p></p> <ul> <li>Continue to run the cells in the section to save the model to Cloud Pak for Data. Once your model is saved, the call to <code>wml_client.repository.list_models()</code> will show it in the output.</li> </ul> <p></p> <p>We've successfully built and saved a machine learning model programmatically. Congratulations!</p>"},{"location":"machine-learning-in-jupyter-notebook/#stop-the-environment","title":"Stop the Environment","text":"<p>Important: In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done.</p> <ul> <li>Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page.</li> </ul> <p></p> <ul> <li>Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the <code>Tool</code> value is <code>Notebook</code>). Click on the three vertical dots at the right of that row and select the <code>Stop</code> option from the menu.</li> </ul> <p></p> <ul> <li>Click the <code>Stop</code> button on the subsequent pop up window.</li> </ul>"},{"location":"machine-learning-in-jupyter-notebook/#conclusion","title":"Conclusion","text":"<p>In this section we covered one approach to building machine learning models on Cloud Pak for Data. We have seen::</p> <ul> <li>How to build a model using Jupyter Notebook</li> <li>Saving models using the Watson Machine Learning SDK.</li> </ul> <p>With this knowledge you should feel right at home within the Jupyter notebook. Moreover, you now know how to build a model and use it in a real life scenario.</p>"},{"location":"openscale-fastpath/","title":"Monitoring models with OpenScale GUI tool using Auto setup","text":"<p>This exercise shows a few of the features of the OpenScale GUI tool. When you first provision Watson OpenScale, either in the IBM Cloud or on Cloud Pak for Data, you will be offered the choice to automatically configure and setup OpenScale. This is called the Auto setup, and it walks the admin through the required steps and loads some sample data to demonstrate the features of OpenScale. We will use this automated Auto setup in this lab. It is presumed that OpenScale Auto setup and Watson Machine Learning have already been configured.</p>"},{"location":"openscale-fastpath/#use-the-insights-dashboard","title":"Use the Insights Dashboard","text":"<ul> <li>To launch the OpenScale service, go the (\u2630) navigation menu and click <code>Services</code> -&gt; <code>Instances</code>.</li> </ul> <ul> <li>Click the 3 horizontal dots next to the OpenScale instance that your Administrator has provisioned and click <code>Open</code>.</li> </ul> <p>Now lets interact with the tools.</p> <ul> <li> <p>OpenScale will load the Insights Dashboard. This will contain tiles for any models being monitored. The tile for <code>GermanCreditRiskModelICP</code> will be the one we will use for this lab, which was configured using the Auto setup script.</p> </li> <li> <p>Click on the left-hand menu icon for <code>Insights</code>, make sure that you are on the <code>Model monitors</code> tab, and then open the tile for the <code>GermanCreditRiskModelICP</code> model (click the 3-dot menu on the tile and then <code>View Details</code>):</p> </li> </ul> <p></p> <ul> <li>Notice the red alert indicators on the various monitors (Fairness, Quality, Drift). You should see a red indicator under Fairness. Click on the Fairness score.</li> </ul> <p></p> <ul> <li> <p>Click on the triangle with <code>!</code> under <code>Fairness</code> -&gt; <code>Sex</code>. This indicates that there has been an alert for this attribute in the <code>Fairness</code> monitor. Alerts are configurable, based on thresholds for fairness outcomes which can be set and altered as desired.</p> </li> <li> <p>By moving your mouse pointer over the trend chart, you can see the values change, and which contains bias. Find and click on a spot in the graph that is below the red threshold line to view details.</p> </li> </ul> <p></p> <ul> <li>Once you click on one of the time periods, you will see details of the Fairness monitor, including a bar chart that shows how many females recieved the \"No Risk\" outcome vs. males. You can click <code>view calculation</code> to see how the fairness score is calculated. Click on <code>View payload transactions</code>.</li> </ul> <p></p> <ul> <li>You will see a list of Transactions. Look for one of the Monitored Group - Female with a \"Group Bias\" check mark and Prediction of \"Risk\". Click <code>Explain prediction</code>. If the time period on the graph for Fairness Monitoring doesn't contain such an element, go back and choose another time period until you can find one. This will make the explanation more interesting.</li> </ul> <p></p> <p>Note: Each of the individual transactions can be examined to see them in detail. Doing so will cache that transaction, as we will see later. Be aware of the fact that the Explainability feature requires 1000's of REST calls to the endpoint using variations of the data that are slightly perturbed, which can require several seconds to complete.</p> <ul> <li>On the <code>Explain</code> tab for this individual transaction, you can see the relative weights of the most important features for this prediction. Examine the data, then click the <code>Inspect</code> tab.</li> </ul> <p></p> <ul> <li>In the <code>Inspect</code> view of this transaction you can see the original features that led to this prediction as well as a series of drop downs and input boxes that offer the ability to change each feature. We can find which features will change the outcome (in this case, from \"Risk\" to \"No Risk\") by clicking the <code>Analysis</code> button. Note that this requires 1000's of REST calls to the endpoint with slight perturbations in the data, so it can take a few minutes. Click the <code>Analysis</code> tab now.</li> </ul> <p></p> <ul> <li>In this particular transaction, we see that the presence of a \"guarantor\" on the loan is the only thing required to flip the outcome from \"Risk\" to \"No Risk\". Other transactions might show a different analysis, so please be aware that your results might vary from this. In the case in this example, you can click the drop down for Others on Loan and change to guarantor. </li> </ul> <p></p> <ul> <li>Choosing this new value for gurantor will expose a button for <code>Score new values</code>. Click this button.</li> </ul> <p></p> <ul> <li>In this example, we can see that the outcome has now been flipped from \"Risk\" to \"No Risk\".</li> </ul> <p></p> <ul> <li>Now, go back to the Insights Dashboard page by clicking on the left-hand menu icon for <code>Insights</code>, make sure that you are on the <code>Model monitors</code> tab. This time open the monitor configuration for the <code>GermanCreditRiskModelICP</code> model by clicking the 3-dot menu on the tile and then <code>Configure monitors</code>.</li> </ul> <p></p> <ul> <li>Click the <code>Endpoints</code> menu on the left, then the Endpoints tab. Use the Endpoint pulldown to select <code>Debiased transactions</code>. This is the REST endpoint that offers a debiased version of the credit risk ML model, based on the features that were configured (i.e. Sex and Age). It will present an inference that attempts to remove the bias that has been detected. </li> </ul> <p></p> <ul> <li> <p>You can see code snippets using cURL, Java, and Python, which can be used in your scripts or applications.</p> </li> <li> <p>Similarly, you can choose the <code>Feedback logging</code> endpoint to get code for Feedback Logging. This provides an endpoint for sending fresh test data for ongoing quality evaluation. You can upload feedback data here or work with your developer to integrate the code snippet provided to publish feedback data to your Watson OpenScale database.</p> </li> </ul>"},{"location":"openscale-fastpath/#using-the-analytics-tools","title":"Using the Analytics tools","text":"<ul> <li>Click on the left-hand menu icon for <code>Insights</code>, make sure that you are on the <code>Model monitors</code> tab, and then open the tile for the <code>GermanCreditRiskModelICP</code> model (click the 3-dot menu on the tile and then <code>View Details</code>):</li> </ul> <ul> <li>Notice the red alert indicators on the various monitors (Fairness, Quality, Drift). You should see a red indicator under Fairness. Click on the <code>Fairness score</code>.</li> </ul> <ul> <li>Click on <code>Analytics</code> -&gt; <code>Predictions by Confidence</code>. It may take a minute or more to create the chart. Here you can see a bar chart that indicates confidence levels and predictions of \"Risk\" and \"No Risk\".</li> </ul> <ul> <li>From this dashboard click on <code>Analytics</code> -&gt; <code>Chart Builder</code>. Here you can create charts using various Measurements, Features, and Dimensions of your machine learning model. You can  see a chart that breaks down <code>Predictions by Confidence</code></li> </ul> <p>Note: You may need to click the date range for 'Past Week' or 'Yesterday' to load the data.</p> <ul> <li>You can experiment with changing the values and examine the charts that are created.</li> </ul> <p></p>"},{"location":"openscale-fastpath/#conclusion","title":"Conclusion","text":"<p>This lab provides a walkthrough of many of the GUI features using the Watson OpenScale tools. The Auto setup deployment creates a machine learning model, deploys it, and inserts historical data to simulate a model that has been used in production over time. The OpenScale monitors are configured, and the user can then explore the various metrics and data. Please continue to explore on your own.</p>"},{"location":"openscale-manual-config/","title":"WARNING: This Module is Unsupported and has been Deprecated.","text":""},{"location":"openscale-manual-config/#trust-in-ai-watson-openscale","title":"Trust in AI &amp; Watson OpenScale","text":"<p>This lab will demonstrate how to monitor your deployed machine learning model using Watson OpenScale. We will run several Jupyter notebooks to show the OpenScale APIs and how they configure various monitors. For each notebook, we'll use the OpenScale GUI tool to explore the results.</p> <p>We'll use several jupyter notebook and instructions:</p> <ul> <li>These instructions for basic OpenScale setup</li> <li>Fairness and Explainiblity monitors</li> <li>Quality monitor and Feedback logging</li> <li>Drift monitor</li> </ul> <p>Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the <code>Workshop Resources</code> -&gt; <code>FAQs / Tips</code> section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the <code>Workshop Resources</code> -&gt; <code>FAQs / Tips</code> section for links to the completed notebooks.</p>"},{"location":"openscale-manual-config/#steps-for-basic-openscale-setup","title":"Steps for basic OpenScale setup","text":"<p>The submodule contains the following steps:</p> <ol> <li>Introduction</li> <li>Open the notebook</li> <li>Run the notebook</li> <li>Begin to Explore the Watson OpenScale UI</li> </ol>"},{"location":"openscale-manual-config/#1-introduction","title":"1. Introduction","text":"<p>Watson OpenScale tracks and measures outcomes from your AI models, and helps ensure they remain fair, explainable and compliant wherever your models were built or are running. OpenScale is designed as an open platform that will operate with various model development environments and various open source tools, including TensorFlow, Keras, SparkML, Seldon, AWS SageMaker, AzureML and more.</p> <p>Watson OpenScale provides a set of monitoring and management tools that help you build trust and implement control and governance structures around your AI investments.</p> <ul> <li>Providing production monitoring for compliance and safeguards \\(auditing model decisions, detecting biases, etc\\)</li> <li>Ensuring that models are resilient to changing situations</li> <li>Aligning model performance with business outcomes</li> </ul> <p>In this lab will walk through the process of deploying a credit risk model and then monitoring the model to explore the different aspects of trusted AI. By the end of the lab, we will have:</p> <ul> <li>Deployed a model from development to a runtime environment.</li> <li>Monitored the performance \\(operational\\) of the model over time.</li> <li>Tracked the model quality \\(accuracy metrics\\) over time.</li> <li>Identified and explored the fairness of the model as it's receiving new data.</li> <li>Understood how the model arrived at its predictions.</li> <li>Tracked the robustness of the model.</li> </ul>"},{"location":"openscale-manual-config/#prerequisites","title":"Prerequisites","text":"<p>It is assumed that an admin has already connected a database to OpenScale, and associated a Machine Learning Provider (in our case, Watson Machine Learning on Cloud Pak for Data).</p> <p>You have already provided a set of sample data to your model when you tested your deployed ML model earlier in the workshop.</p> <p>For example, using the UI to test the deployed model, or using cURL or the Python app. Do this now if you have not already run a test.</p>"},{"location":"openscale-manual-config/#2-open-the-notebook","title":"2. Open the notebook","text":"<ul> <li>Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project.</li> </ul> <ul> <li> <p>From your Project overview page, click on the <code>Assets</code> tab to open the assets page where your project assets are stored and organized.</p> </li> <li> <p>Scroll down to the <code>Notebooks</code> section of the page and Click on the pencil icon at the right of the <code>openscale-initial-setup</code> notebook.</p> </li> </ul> <p></p> <ul> <li>When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.</li> </ul> <p></p>"},{"location":"openscale-manual-config/#3-run-the-notebook","title":"3. Run the Notebook","text":"<p>Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do.</p> <p>You will run cells individually by highlighting each cell, then either click the <code>Run</code> button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk (<code>[*]</code>) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. <code>[17]</code>).</p> <p>Please note that there are several places in the notebook where you need to update variables. Some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. These changes are described below.</p>"},{"location":"openscale-manual-config/#wos_credentials","title":"WOS_CREDENTIALS","text":"<ul> <li> <p>In the notebook section 2.0  you will add your Cloud Pak for Data platform credentials for the WOS_CREDENTIALS.</p> </li> <li> <p>For the <code>url</code>, use the URL your Cloud Pak for Data cluster, i.e something like: <code>\"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\"</code></p> </li> <li>For the <code>username</code>, use your Cloud Pak for Data login username.</li> <li>For the <code>password</code>, user your Cloud Pak for Data login password.</li> </ul>"},{"location":"openscale-manual-config/#model_name","title":"MODEL_NAME","text":"<ul> <li> <p>After running cell 5.1 containing <code>ai_client.data_mart.bindings.list_assets()</code> you should see the machine learning model that you deployed previously in the workshop.</p> </li> <li> <p>You will add the name of this model in cell 5.2 as MODEL_NAME</p> </li> </ul>"},{"location":"openscale-manual-config/#default_space","title":"default_space","text":"<ul> <li> <p>After running cell 6.1 containing <code>wml_client.spaces.list()</code> you should see the name of your deployment space.</p> </li> <li> <p>Use the GUID of this deployment space in cell 6.2 as default_space.</p> </li> </ul>"},{"location":"openscale-manual-config/#deployment_name","title":"DEPLOYMENT_NAME","text":"<ul> <li>In section 6.3 set the DEPLOYMENT_NAME to the name that you gave to your Online deployment for the model during the 'Deploy and Test Machine Learning Modles' portion of the workshop.</li> </ul>"},{"location":"openscale-manual-config/#4-begin-to-explore-the-watson-openscale-ui","title":"4. Begin to Explore the Watson OpenScale UI","text":"<p>Now that you have created a machine learning model and configured OpenScale with a subscription to that model deployment, you can utilize the OpenScale dashboard to monitor the model. Although we have not enabled any type of monitoring yet, with the deployment approach we are using for this lab \\( Watson Machine Learning as the model engine \\), we will be able to see payload and some performance information out of the box.</p> <ul> <li>In the same browser \\(but a separate tab\\), open the <code>Services</code> tab by clicking the <code>Services</code> icon on the top right.</li> </ul> <p></p> <ul> <li>Find and click on the <code>Watson OpenScale</code> tile.</li> </ul> <p></p> <ul> <li>Launch the OpenScale UI tooling by clicking on the <code>Launch</code> button</li> </ul> <p></p> <ul> <li>When the Insights Dashboard loads, Click on the 'Model Monitors'  tab. Here you will see tiles for all model subscriptions that are being monitored including the deployment you configured in the jupyter notebook when you ran it in the previous section:</li> </ul> <p></p> <p>Note:Do not worry if the name you see does not match exactly with the screenshot. The subscription name you see will correspond to the variable used in the Jupyter notebook and the name you used when you deployed the model. At this point, it is normal for the subscription tile to show no monitors have ben configured (i.e N/A under Quality, Fairness, Drift)</p>"},{"location":"openscale-manual-config/#confidence-distribution","title":"Confidence Distribution","text":"<ul> <li>From the 'Model Monitors' tab, in the subscription tile you have created, click on one of the <code>N/A</code> values (i.e the <code>N/A</code> under the 'Fairness' heading). You will see some Analytics data, with the Date Range set to Today. We've just configured OpenScale to monitor our deployment, and sent a scoring request with 8 records, so there is not much here yet. We can see the distribution of confidence for those 8 predictions.</li> </ul> <ul> <li>If you hover over the bars you will see the number of 'Risk' and 'No Risk' predictions for each confidence range.</li> </ul>"},{"location":"openscale-manual-config/#chart-builder","title":"Chart Builder","text":"<ul> <li> <p>Some additional data is present in the Chart Builder tab.</p> </li> <li> <p>Click on <code>Analytics</code> -&gt; <code>Chart Builder</code>. Here you can create charts using various Measurements, Features, and Dimensions of your machine learning model. Experiment with different values from the drop downs and examine the charts that are created.</p> </li> </ul> <p></p>"},{"location":"openscale-manual-config/#stop-the-environment","title":"Stop the Environment","text":"<p>Important: When you have completed the last submodule in this \"openscale-manual-config\" section that you will be doing, it's recommended you stop the environment in order to conserve resources. You should only follow these steps to stop your environment if you are not going to proceed with the other sub-modules in this section.</p> <ul> <li>Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page.</li> </ul> <p></p> <ul> <li>Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the <code>Tool</code> value is <code>Notebook</code>). Click on the three vertical dots at the right of that row and select the <code>Stop</code> option from the menu.</li> </ul> <p></p> <ul> <li>Click the <code>Stop</code> button on the subsequent pop up window.</li> </ul>"},{"location":"openscale-manual-config/#conclusion","title":"Conclusion","text":"<p>We begun the process of monitoring our machine learning deployment with openscale. At this point we have just a subscription from OpenScale for our deployed model.</p> <p>Proceed to the next sub-module to configure the Fairness and Explainability monitors.</p>"},{"location":"openscale-manual-config/DRIFT/","title":"WARNING: This Module is Unsupported and has been Deprecated.","text":""},{"location":"openscale-manual-config/DRIFT/#configuring-drift-monitor-for-openscale","title":"Configuring Drift Monitor for OpenScale","text":"<p>Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the <code>Workshop Resources</code> -&gt; <code>FAQs / Tips</code> section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the <code>Workshop Resources</code> -&gt; <code>FAQs / Tips</code> section for links to the completed notebooks.</p>"},{"location":"openscale-manual-config/DRIFT/#steps-for-drift-monitor-configuration","title":"Steps for Drift Monitor Configuration","text":"<p>The submodule contains the following steps:</p> <ol> <li>Open the notebook</li> <li>Run the notebook</li> <li>Look at Drift in the Dashboard</li> </ol>"},{"location":"openscale-manual-config/DRIFT/#1-open-the-notebook","title":"1. Open the notebook","text":"<p>If you Created the Project using the CreditRiskProject.zip file, your notebook will be present in that project.</p> <ul> <li>Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project.</li> </ul> <p></p> <ul> <li> <p>From your Project overview page, click on the <code>Assets</code> tab to open the assets page where your project assets are stored and organized.</p> </li> <li> <p>Scroll down to the <code>Notebooks</code> section of the page and Click on the pencil icon at the right of the <code>openscale-drift-config</code> notebook.</p> </li> </ul> <p></p> <ul> <li>When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.</li> </ul> <p></p>"},{"location":"openscale-manual-config/DRIFT/#2-run-the-notebook","title":"2. Run the Notebook","text":"<p>Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do.</p> <p>You will run cells individually by highlighting each cell, then either click the <code>Run</code> button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk (<code>[*]</code>) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. <code>[17]</code>).</p> <p>Please note that there are several places in the notebook where you need to update variables. Some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. These changes are described below.</p>"},{"location":"openscale-manual-config/DRIFT/#wos_credentials","title":"WOS_CREDENTIALS","text":"<ul> <li> <p>In the notebook section 2.0  you will add your Cloud Pak for Data platform credentials for the WOS_CREDENTIALS.</p> </li> <li> <p>For the <code>url</code>, use the URL your Cloud Pak for Data cluster, i.e something like: <code>\"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\"</code></p> </li> <li>For the <code>username</code>, use your Cloud Pak for Data login username.</li> <li>For the <code>password</code>, user your Cloud Pak for Data login password.</li> </ul>"},{"location":"openscale-manual-config/DRIFT/#3-look-at-drift-in-the-dashboard","title":"3. Look at Drift in the Dashboard","text":"<ul> <li>In the same browser \\(but a separate tab\\), open the <code>Services</code> tab by clicking the <code>Services</code> icon on the top right.</li> </ul> <ul> <li>Find and click on the <code>Watson OpenScale</code> tile.</li> </ul> <ul> <li>Launch the OpenScale UI tooling by clicking on the <code>Launch</code> button</li> </ul> <ul> <li>When the dashboard loads, Click on the 'Model Monitors'  tab and you will see the deployment you configured in the jupyter notebook when you ran it in the previous section. Click on the <code>Drift</code> section of the tile to bring up the Drift monitor.</li> </ul> <p>Note: Do not worry if the name you see does not match exactly with the screenshot. The deployment name you see will correspond to the variable used in the Jupyter notebook. Note: If you click on the card itself, you can get to the drift monitor page by clicking on the percentage shown for drift in the UI.</p> <ul> <li>Click on <code>Drop in accuracy</code>and then look for a time slot on the graph that shows bias (i.e. below the red threshold line). The monitor only runs every three hours, so there may only be one teal colored \"dot\" representing a single run when you first visit the graph. Click on it for more details that will show the number of transactions responsible \"for drop in accuracy\" and \"for drop in data consistency\".</li> </ul> <p></p> <ul> <li>You can choose to get details about \"Transactions responsible for drop in accuracy and data consistency\":</li> </ul> <p></p> <ul> <li>From here you can see groupings of transactions that caused drift. Where the groups are formed by shared features. Drilling deeper will bring up the individual transactions, and as we've seen before, we can choose a transaction to get the details.</li> </ul> <p></p>"},{"location":"openscale-manual-config/DRIFT/#stop-the-environment","title":"Stop the Environment","text":"<p>Important: When you have completed the last submodule in this \"openscale-manual-config\" section that you will be doing, it's recommended you stop the environment in order to conserve resources. You should only follow these steps to stop your environment if you are not going to proceed with the other sub-modules in this section.</p> <ul> <li>Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page.</li> </ul> <p></p> <ul> <li>Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the <code>Tool</code> value is <code>Notebook</code>). Click on the three vertical dots at the right of that row and select the <code>Stop</code> option from the menu.</li> </ul> <p></p> <ul> <li>Click the <code>Stop</code> button on the subsequent pop up window.</li> </ul>"},{"location":"openscale-manual-config/DRIFT/#conclusion","title":"Conclusion","text":"<p>We've seen how to configure Drift monitoring using a Jupyter notebook. Next, we'll add some historical data to emulate what would happen for a Machine learning model that is deployed in production, monitored with OpenScale, and continually receiving scoring requests.</p> <p>Proceed to the next sub-module to load historical data</p>"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/","title":"WARNING: This Module is Unsupported and has been Deprecated.","text":""},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#configure-fairness-and-explainability-monitors-for-openscale","title":"Configure Fairness and Explainability monitors for OpenScale","text":"<p>Watson OpenScale utilizes several monitors to gather data about machine learning inferences and the GUI tool can then present that data in a form that is useful. In this sub-module we will use a Jupyter notebook to configure the monitor for Fairness, allowing us to choose a feature to monitor. In our loan risk scenario, we'll monitor the Risk feature and look at 2 groups. For Gender, we'll designate <code>Male</code> as the majority group and <code>Female</code> as the minority group, and then use the Fairness monitor to make sure that the majority group does not recieve a favorable outcome more often than the minority group, within a tolerance of 5%. We'll also designate the Age group of people 18-25 the minority and 26-75 the majority to look for bias against those in the minority age cohort.</p> <p>We'll then enable the Explainability monitor, which allows us to then use the API or GUI tool to explain individual transactions. By sending slightly perturbed data to the scoring endpoint, the explainability algorithm can build a model of which features contributed to the category of Risk or No Risk, and give a quantitative breakdown of the contributions of each feature to the results.</p> <p>Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the <code>Workshop Resources</code> -&gt; <code>FAQs / Tips</code> section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the <code>Workshop Resources</code> -&gt; <code>FAQs / Tips</code> section for links to the completed notebooks.</p>"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#steps-for-openscale-fairness-and-explainabilty-monitor-setup","title":"Steps for OpenScale Fairness and Explainabilty monitor setup","text":"<p>The submodule contains the following steps:</p> <ol> <li>Open the notebook</li> <li>Run the notebook</li> <li>Begin to Explore the Watson OpenScale UI</li> </ol>"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#1-open-the-notebook","title":"1. Open the notebook","text":"<ul> <li>Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project.</li> </ul> <ul> <li> <p>From your Project overview page, click on the <code>Assets</code> tab to open the assets page where your project assets are stored and organized.</p> </li> <li> <p>Scroll down to the <code>Notebooks</code> section of the page and Click on the pencil icon at the right of the <code>openscale-fairness-explainability</code> notebook.</p> </li> </ul> <p></p> <ul> <li>When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.</li> </ul> <p></p>"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#2-run-the-notebook","title":"2. Run the Notebook","text":"<p>Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do.</p> <p>You will run cells individually by highlighting each cell, then either click the <code>Run</code> button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk (<code>[*]</code>) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. <code>[17]</code>).</p> <p>Please note that there are several places in the notebook where you need to update variables. Some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. These changes are described below.</p>"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#wos_credentials","title":"WOS_CREDENTIALS","text":"<ul> <li> <p>In the notebook section 2.0  you will add your Cloud Pak for Data platform credentials for the WOS_CREDENTIALS.</p> </li> <li> <p>For the <code>url</code>, use the URL your Cloud Pak for Data cluster, i.e something like: <code>\"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\"</code></p> </li> <li>For the <code>username</code>, use your Cloud Pak for Data login username.</li> <li>For the <code>password</code>, user your Cloud Pak for Data login password.</li> </ul>"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#get-explainability-transaction_id","title":"Get Explainability transaction_id","text":"<ul> <li> <p>In order to use the Explainability feature, we will need the ID for an individual transaction.</p> </li> <li> <p>In the notebook, after running cell 5.5 Run explanation for sample record , the output will print a <code>transaction_id</code>. Copy this id.</p> </li> </ul>"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#3-begin-to-explore-the-watson-openscale-ui","title":"3. Begin to Explore the Watson OpenScale UI","text":"<p>We've enabled the monitors for Fairness and Explainability, now let's explore the results in the OpenScale GUI.</p> <ul> <li>In the same browser \\(but a separate tab\\), open the <code>Services</code> tab by clicking the <code>Services</code> icon on the top right.</li> </ul> <p></p> <ul> <li>Find and click on the <code>Watson OpenScale</code> tile.</li> </ul> <p></p> <ul> <li>Launch the OpenScale UI tooling by clicking on the <code>Launch</code> button</li> </ul> <p></p> <ul> <li>When the dashboard loads, Click on the 'Model Monitors'  tab and you will see the deployment you configured in the jupyter notebook when you ran it in the previous section. Click on the <code>Fairness</code> section of the tile to bring up the Fairness monitor.</li> </ul> <p></p> <p>Note: Do not worry if the name you see does not match exactly with the screenshot. The deployment name you see will correspond to the variable used in the Jupyter notebook.</p>"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#fairness-monitor","title":"Fairness Monitor","text":"<ul> <li> <p>You will see that the Fairness monitor is enabled and shows a graph of recent transactions. Details for the Fairness monitor are on the left side of the graph, including the Threshold to trigger a bias alert, details of the monitored groups, and Schedule information. You can hover over sections of the graph to see the results of each hour and the scoring requests that were performed.</p> </li> <li> <p>Look for a time slot that shows bias (i.e. below the red threshold line). The monitor only runs once per hour, so there may only be one teal colored \"dot\" representing a single run when you first visit the graph. Click on it for more details:</p> </li> </ul> <p></p> <ul> <li> <p>For a given time slot, we will see a comparison between the 2 groups, in this case, female and male. A bar chart presents information for percent of favorable vs. unfavorable outcomes.</p> </li> <li> <p>We can choose our Data set from a series of radio buttons, whether Payload + Perturbed, Payload, Training, or Debiased. The Monitored features can be toggled, and we can change the Date and time. Click on the various radio buttons, toggle the monitored features, and change the time to see how the tool works:</p> </li> </ul> <p></p> <ul> <li>Now back to our biased time slot, click on Debiased Data Set and Sex and view how the use of the Debiased endpoint has made our scoring more fair:</li> </ul> <p></p> <ul> <li>Click on the <code>View Debiased Endpoint</code> button. Here you can see some Debiased Endpoint Code Snippet examples, showing cURL, Java, and Python code that can utilize the endpoint for debiased transactions, enabling a developer to get a machine learning model score that prevents the biased outcome. (You may need to change the 'Code language' drop down list to see code snippets in different languages):</li> </ul> <p></p> <ul> <li> <p>Click the <code>Back</code> button on your browser to go back to the Transactions details for the Fairness monitor page.</p> </li> <li> <p>Click on the <code>View Transactions</code> button. We can see the various transactions that took place during this time slot, as well as some aggregate information around \"Risk\" and \"No Risk\" for both the Current model and the Debiased model. We can click on <code>Explain</code> for one of the transactions for more detailed information (HINT: A \"Risk\" transaction might be more interesting):</p> </li> </ul> <p></p> <p>Note that the explanation of a transaction requires that 1000's of scoring transactions take place using slightly perturbed data for each of the features. This can take several seconds, or even minutes. Future use of this individual transaction will be cached, so the network latency and overhead from performing the scoring will not be a factor.</p> <ul> <li>There is a lot of information available for a single transaction:</li> </ul> <p></p> <ul> <li> <p>Click the \"i\" information icons next to Minimum changes for another outcome and Maximum changes allowed for the same outcome to help understand the use of the Pertinent Negative and Pertinent Positive:</p> </li> <li> <p>You can see information about the Pertinent Negative, for example, that shows what the minimum changes would need to be to cause a different outcome, i.e from Risk to No Risk, or changing No Risk to Risk. In my example shown, you can see that this is not always calculated:</p> </li> </ul> <p></p> <ul> <li>Scroll down and you can see that the Most important factors influencing prediction are highlighted, and below is a complet breakdown of all features, with the percent of influence for the score of either Risk or No Risk:</li> </ul> <p></p> <ul> <li> <p>(Optional) If you saved the transaction_id that you copied after running cell 5.5 in the notebook, you can past it into the search bar and press enter:</p> </li> <li> <p>Any previous Explainability transactions will be cached and presented as a tab that you can click on to revisit in the future.</p> </li> </ul>"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#stop-the-environment","title":"Stop the Environment","text":"<p>Important: When you have completed the last submodule in this \"openscale-manual-config\" section that you will be doing, it's recommended you stop the environment in order to conserve resources. You should only follow these steps to stop your environment if you are not going to proceed with the other sub-modules in this section.</p> <ul> <li>Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page.</li> </ul> <p></p> <ul> <li>Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the <code>Tool</code> value is <code>Notebook</code>). Click on the three vertical dots at the right of that row and select the <code>Stop</code> option from the menu.</li> </ul> <p></p> <ul> <li>Click the <code>Stop</code> button on the subsequent pop up window.</li> </ul>"},{"location":"openscale-manual-config/FAIRNESS-EXPLAINABILITY-README/#conclusion","title":"Conclusion","text":"<p>You were able to configure Fairness monitoring of your machine learning deployment using the OpenScale python SDK and the APIs in a Juypyter notebook. You then scored 200 randomly selected records to provide enough information to calculate fairness, and run the monitor. After that, Explainability was enabled and an indiviudal transaction selected for later inspection.</p> <p>The OpenScale GUI tool was utilized to look at the Fairness monitor and the Explainability tool. Later in this workshop we will add more historical data to simulate a machine learning model deployed in production and the data available from these monitors.</p> <p>Proceed to the next sub-module to configure Quality monitor and Feedback logging</p>"},{"location":"openscale-manual-config/HISTORIC-DATA-README/","title":"WARNING: This Module is Unsupported and has been Deprecated.","text":""},{"location":"openscale-manual-config/HISTORIC-DATA-README/#load-historic-data-for-openscale","title":"Load Historic Data for OpenScale","text":"<p>For a deployed machine learning model, OpenScale will record all of the requests for scoring and the results in the datamart using feedback logging. In this submodule, we'll emulate a production system that has been used for a week to score many requests, allowing the various configured monitors to present some interesting data. Note that this Historic Data submodule can be run at any time.</p> <p>Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the <code>Workshop Resources</code> -&gt; <code>FAQs / Tips</code> section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the <code>Workshop Resources</code> -&gt; <code>FAQs / Tips</code> section for links to the completed notebooks.</p>"},{"location":"openscale-manual-config/HISTORIC-DATA-README/#steps-for-historical-data-load","title":"Steps for Historical Data Load","text":"<p>The submodule contains the following steps:</p> <ol> <li>Open the notebook</li> <li>Run the notebook</li> <li>Explore the Watson OpenScale UI</li> </ol>"},{"location":"openscale-manual-config/HISTORIC-DATA-README/#1-open-the-notebook","title":"1. Open the notebook","text":"<ul> <li>Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project.</li> </ul> <ul> <li> <p>From your Project overview page, click on the <code>Assets</code> tab to open the assets page where your project assets are stored and organized.</p> </li> <li> <p>Scroll down to the <code>Notebooks</code> section of the page and Click on the pencil icon at the right of the <code>openscale-historic-data</code> notebook.</p> </li> </ul> <p></p> <ul> <li>When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.</li> </ul> <p></p>"},{"location":"openscale-manual-config/HISTORIC-DATA-README/#2-run-the-notebook","title":"2. Run the Notebook","text":"<p>Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do.</p> <p>You will run cells individually by highlighting each cell, then either click the <code>Run</code> button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk (<code>[*]</code>) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. <code>[17]</code>).</p> <p>Please note that there are several places in the notebook where you need to update variables. Some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. These changes are described below.</p>"},{"location":"openscale-manual-config/HISTORIC-DATA-README/#wos_credentials","title":"WOS_CREDENTIALS","text":"<ul> <li> <p>In the notebook section 2.0  you will add your Cloud Pak for Data platform credentials for the WOS_CREDENTIALS.</p> </li> <li> <p>For the <code>url</code>, use the URL your Cloud Pak for Data cluster, i.e something like: <code>\"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\"</code></p> </li> <li>For the <code>username</code>, use your Cloud Pak for Data login username.</li> <li>For the <code>password</code>, user your Cloud Pak for Data login password.</li> </ul>"},{"location":"openscale-manual-config/HISTORIC-DATA-README/#3-explore-the-watson-openscale-ui","title":"3. Explore the Watson OpenScale UI","text":"<p>Now that we've simulated a Machine Learning deployment in production, we can look at the associated monitors again and see more detail. Re-visit the various monitors and look again at the graphs, charts and explanations after the addition of the historical data:</p> <ul> <li> <p>Fairness monitor and Explainability</p> </li> <li> <p>Quality monitor and Feedback logging</p> </li> <li> <p>Drift monitor</p> </li> </ul>"},{"location":"openscale-manual-config/HISTORIC-DATA-README/#stop-the-environment","title":"Stop the Environment","text":"<p>Important: When you have completed the last submodule in this \"openscale-manual-config\" section that you will be doing, it's recommended you stop the environment in order to conserve resources. You should only follow these steps to stop your environment if you are not going to proceed with the other sub-modules in this section.</p> <ul> <li>Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page.</li> </ul> <p></p> <ul> <li>Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the <code>Tool</code> value is <code>Notebook</code>). Click on the three vertical dots at the right of that row and select the <code>Stop</code> option from the menu.</li> </ul> <p></p> <ul> <li>Click the <code>Stop</code> button on the subsequent pop up window.</li> </ul>"},{"location":"openscale-manual-config/HISTORIC-DATA-README/#conclusion","title":"Conclusion","text":"<p>With the addition of historical data, we can now use the OpenScale tools in a simulated production environment. We can look at Fairness, Explainability, Quality, and Drift, and see how all transactions are logged. This workshop contains API code, configuration tools, and details around using the UI tool to enable a user to monitor production machine learning environments.</p>"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/","title":"WARNING: This Module is Unsupported and has been Deprecated.","text":""},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#configure-quality-monitoring-and-feedback-logging","title":"Configure Quality monitoring and Feedback logging","text":"<p>Watson OpenScale utilizes several monitors to gather data about machine learning inferences and the GUI tool can then present that data in a form that is useful. In this sub-module we will use a Jupyter notebook to configure the monitor for Quality and enable Feedback logging.</p> <p>Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the <code>Workshop Resources</code> -&gt; <code>FAQs / Tips</code> section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the <code>Workshop Resources</code> -&gt; <code>FAQs / Tips</code> section for links to the completed notebooks.</p>"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#steps-for-openscale-quality-monitor-and-feedback-logging","title":"Steps for OpenScale Quality monitor and Feedback logging","text":"<p>The submodule contains the following steps:</p> <ol> <li>Open the notebook</li> <li>Run the notebook</li> <li>Begin to Explore the Watson OpenScale UI</li> </ol>"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#1-open-the-notebook","title":"1. Open the Notebook","text":"<ul> <li>Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project.</li> </ul> <ul> <li> <p>From your Project overview page, click on the <code>Assets</code> tab to open the assets page where your project assets are stored and organized.</p> </li> <li> <p>Scroll down to the <code>Notebooks</code> section of the page and Click on the pencil icon at the right of the <code>openscale-quality-feedback</code> notebook.</p> </li> </ul> <p></p> <ul> <li>When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.</li> </ul> <p></p>"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#2-run-the-notebook","title":"2. Run the Notebook","text":"<p>Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do.</p> <p>You will run cells individually by highlighting each cell, then either click the <code>Run</code> button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk (<code>[*]</code>) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. <code>[17]</code>).</p> <p>Please note that there are several places in the notebook where you need to update variables. Some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell. These changes are described below.</p>"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#wos_credentials","title":"WOS_CREDENTIALS","text":"<ul> <li> <p>In the notebook section 2.0  you will add your Cloud Pak for Data platform credentials for the WOS_CREDENTIALS.</p> </li> <li> <p>For the <code>url</code>, use the URL your Cloud Pak for Data cluster, i.e something like: <code>\"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\"</code></p> </li> <li>For the <code>username</code>, use your Cloud Pak for Data login username.</li> <li>For the <code>password</code>, user your Cloud Pak for Data login password.</li> </ul>"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#3-begin-to-explore-the-watson-openscale-ui","title":"3. Begin to Explore the Watson OpenScale UI","text":"<ul> <li> <p>We've enabled the monitor for Quality and Feedback logging, now let's explore the results in the OpenScale GUI.</p> </li> <li> <p>In the same browser \\(but a separate tab\\), open the <code>Services</code> tab by clicking the <code>Services</code> icon on the top right.</p> </li> </ul> <p></p> <ul> <li>Find and click on the <code>Watson OpenScale</code> tile.</li> </ul> <p></p> <ul> <li>Launch the OpenScale UI tooling by clicking on the <code>Launch</code> button</li> </ul> <p></p> <ul> <li>When the dashboard loads, Click on the 'Model Monitors'  tab and you will see the deployment you configured in the jupyter notebook when you ran it in the previous section. Click on the <code>Quality</code> section of the tile to bring up the Fairness monitor.</li> </ul> <p></p> <p>Note: Do not worry if the name you see does not match exactly with the screenshot. The deployment name you see will correspond to the variable used in the Jupyter notebook.</p>"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#quality-monitor","title":"Quality Monitor","text":"<ul> <li> <p>In our dashboard we can see that we have a choice for a variety of graphs under Quality. If we choose Area under ROC, where there is a threshold violation in my example, we'll see a limited chart due to the lack of scoring data. (More data will be added later to make this more interesting).</p> </li> <li> <p>Look for a time slot that shows a quality alert (i.e. below the red threshold line). The monitor only runs once per hour, so there may only be one teal colored \"dot\" representing a single run when you first visit the graph. Click on it for more details.</p> </li> </ul> <p></p> <ul> <li>We can see statistics for this time slot including Area under ROC, TPR, FPR, Recall, Precision, and more:</li> </ul> <p></p> <ul> <li>Other time slots can be examined to gather the relevant quality statistics.</li> </ul>"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#stop-the-environment","title":"Stop the Environment","text":"<p>Important: When you have completed the last submodule in this \"openscale-manual-config\" section that you will be doing, it's recommended you stop the environment in order to conserve resources. You should only follow these steps to stop your environment if you are not going to proceed with the other sub-modules in this section.</p> <ul> <li>Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page.</li> </ul> <p></p> <ul> <li>Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the <code>Tool</code> value is <code>Notebook</code>). Click on the three vertical dots at the right of that row and select the <code>Stop</code> option from the menu.</li> </ul> <p></p> <ul> <li>Click the <code>Stop</code> button on the subsequent pop up window.</li> </ul>"},{"location":"openscale-manual-config/QUALITY-FEEDBACK-README/#conclusion","title":"Conclusion","text":"<p>In this sub-module we've setup Payload logging and the Quality monitor.</p> <p>Proceed to the next sub-module to configure the Drift monitor</p>"},{"location":"openscale-notebook/","title":"Configure OpenScale in a Jupyter Notebook","text":"<p>There are several ways of configuring Watson OpenScale to monitor machine learning deployments, including the automatic configuration, using the GUI tool, a more manual configuration using the APIs, and some combintation of these. For this exercise we're going to configure our OpenScale service by running a Jupyter Notebook. This provides examples of using the OpenScale Python APIs programatically.</p> <p>Note: It is assumed that you have followed the instructions in the pre-work section to create a project based on an existing project file. If you did not use the project import or do not see the Jupyter notebooks mentioned in this module, see the <code>Workshop Resources</code> -&gt; <code>FAQs / Tips</code> section for instructions to import the necessary notebooks. Also note that the Jupyter notebooks included in the project have been cleared of output. If you would like to see the notebook that has already been completed with output, see the <code>Workshop Resources</code> -&gt; <code>FAQs / Tips</code> section for links to the completed notebooks.</p>"},{"location":"openscale-notebook/#1-open-the-notebook","title":"1. Open the Notebook","text":"<ul> <li>Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project.</li> </ul> <ul> <li> <p>From your Project overview page, click on the <code>Assets</code> tab to open the assets page where your project assets are stored and organized.</p> </li> <li> <p>Scroll down to the <code>Notebooks</code> section of the page and Click on the pencil icon at the right of the <code>openscale-full-configuration</code> notebook.</p> </li> </ul> <p></p> <ul> <li>When the Jupyter notebook is loaded and the kernel is ready, we will be ready to start executing it in the next section.</li> </ul> <p></p>"},{"location":"openscale-notebook/#2-update-credentials","title":"2. Update Credentials","text":"<ul> <li> <p>In the notebook section 1.2 you edit the first code cell to use your Cloud Pak for Data platform credentials in the <code>WOS_CREDENTIALS</code>.</p> </li> <li> <p>For the <code>url</code>, use the URL your cluster, i.e something like: <code>\"url\": \"https://zen.clusterid.us-south.containers.appdomain.cloud\"</code>.</p> </li> <li>For the <code>username</code>, use your login username.</li> <li>For the <code>password</code>, user your login password.</li> </ul> <p></p>"},{"location":"openscale-notebook/#3-run-the-notebook","title":"3. Run the Notebook","text":"<p>Spend some time looking through the sections of the notebook to get an overview. A notebook is composed of text (markdown or heading) cells and code cells. The markdown cells provide comments on what the code is designed to do.</p> <p>You will run cells individually by highlighting each cell, then either click the <code>Run</code> button at the top of the notebook or hitting the keyboard short cut to run the cell (Shift + Enter but can vary based on platform). While the cell is running, an asterisk (<code>[*]</code>) will show up to the left of the cell. When that cell has finished executing a sequential number will show up (i.e. <code>[17]</code>).</p> <p>Please note that some of the comments in the notebook are directions for you to modify specific sections of the code. Perform any changes as indicated before running / executing the cell.</p>"},{"location":"openscale-notebook/#get-transactions-for-explainability","title":"Get Transactions for Explainability","text":"<p>Under <code>7.8 Identify transactions for Explainability</code> run the cell. It will produce a series of UIDs for indidvidual ML scoring transactions. Copy one or more of them to examine in the next section.</p>"},{"location":"openscale-notebook/#stop-the-environment","title":"Stop the Environment","text":"<p>Important: In order to conserve resources, make sure that you stop the environment used by your notebook(s) when you are done.</p> <ul> <li>Navigate back to your project information page by clicking on your project name from the navigation drill down on the top left of the page.</li> </ul> <p></p> <ul> <li>Click on the 'Environments' tab near the top of the page. Then in the 'Active environment runtimes' section, you will see the environment used by your notebook (i.e the <code>Tool</code> value is <code>Notebook</code>). Click on the three vertical dots at the right of that row and select the <code>Stop</code> option from the menu.</li> </ul> <p></p> <ul> <li>Click the <code>Stop</code> button on the subsequent pop up window.</li> </ul>"},{"location":"openscale-notebook/#conclusion","title":"Conclusion","text":"<p>In this section we covered one of the approaches to configure Watson OpenScale to monitor a machine learning model on Cloud Pak for Data. We have seen:</p> <ul> <li>How to build a model using Jupyter Notebook.</li> <li>How to use the OpenScale Python APIs programatically.</li> <li>How to configure all the monitors in OpenScale.</li> </ul>"},{"location":"pre-work/","title":"Pre-work","text":"<p>Before we get started, we will download some assets and complete some setup for our workshop.</p>"},{"location":"pre-work/#1-download-workshop-assets","title":"1. Download Workshop Assets","text":"<p>Various parts of this workshop will require the attendee to upload files or run scripts. These artifacts have been collected in the following zip files which you can download by clicking the link below and saving the zip files locally to your machine.</p> <ul> <li>Cloud Pak for Data Analytics Project</li> <li>Sample Python Application</li> </ul> <p>Note: The analytics project zip file does not to be unzipped/expanded. It will be directly uploaded to the Cloud Pak for Data platform as a zip file. For reference, all these assets are also in the GitHub repo for this workshop.</p>"},{"location":"pre-work/#2-create-a-project-and-deployment-space","title":"2. Create a Project and Deployment Space","text":"<p>At this point of the workshop we will be using Cloud Pak for Data for the remaining steps.</p> <ul> <li>Launch a browser and navigate to your Cloud Pak for Data deployment.</li> </ul> <p>NOTE: Your instructor will provide a URL and credentials to log into Cloud Pak for Data!</p> <p></p>"},{"location":"pre-work/#create-a-new-project","title":"Create a New Project","text":"<p>In Cloud Pak for Data, we use the concept of a project to collect / organize the resources used to achieve a particular goal (resources to build a solution to a problem). Your project resources can include data, collaborators, and analytic assets like notebooks and models, etc.</p> <ul> <li>Go the (\u2630) navigation menu and under the Projects section click on <code>All Projects</code>.</li> </ul> <p></p> <ul> <li>Click on the <code>New project</code> button on the top right.</li> </ul> <p></p> <ul> <li>We are going to create a project from an existing file (which contains the assets we will use throughout this workshop), as opposed to creating an empty project. Select the <code>Create a project from a file</code> option.</li> </ul> <p></p> <ul> <li>Click on the <code>browse</code> link and in the file browser pop-up, navigate to where you downloaded the <code>CreditRiskProject.zip</code> file in the previous section, then click the <code>open</code> button.</li> </ul> <p></p> <ul> <li>Give the project a name and click the <code>Create</code> button.</li> </ul> <p></p> <ul> <li>From the project creation succesfully created pop up window, click on the <code>View new project</code> button.</li> </ul> <p></p>"},{"location":"pre-work/#create-a-deployment-space","title":"Create a Deployment Space","text":"<p>Cloud Pak for Data uses the concept of <code>Deployment Spaces</code> to configure and manage the deployment of a set of related deployable assets. These assets can be data files, machine learning models, etc.</p> <ul> <li>Go the (\u2630) navigation menu and click <code>Deployments</code>.</li> </ul> <p></p> <ul> <li>Click on the <code>New deployment space</code> button.</li> </ul> <p></p> <ul> <li>Give your deployment space a unique name, optional description, optional deployment stage, then click the <code>Create</code> button.</li> </ul> <p></p> <ul> <li>From the deployment space creation pop up window, click on the <code>View new space</code> button.</li> </ul> <p></p> <ul> <li> <p>Next, we will add a collaborator to the new deployment space. Collaborators allow others to view/edit/manage the assets being deployed. In this workshop, we want the models we deploy to be visible and monitored in the OpenScale model monitoring lab.</p> </li> <li> <p>Click on the <code>Manage</code> tab, then click on <code>Access control</code> on the left, and then on <code>Add collaborators</code> and <code>Add users</code> on the right.</p> </li> </ul> <p></p> <ul> <li>Enter \"admin\" as a Collaborator input field and select the <code>Admin</code> user from the drop down list. Then click on the <code>Add to list</code> button.</li> </ul> <p></p> <p>NOTE: We are adding the user that configured the machine learning instance for OpenScale monitoring. In this case, the user is the admin user.</p> <ul> <li>Click the <code>Add</code> button to finish adding the collaborator. You should be brought back to the deployment space page and see your user ID along with the <code>Admin</code> user as collaborators for this space.</li> </ul>"},{"location":"pre-work/#conclusion","title":"Conclusion","text":"<p>We've completed creating the project and deployment space that we will be using for the rest of this workshop.</p>"},{"location":"watson-knowledge-catalog-admin/","title":"Watson Knowledge Catalog for Admins","text":"<p>This exercise demonstrates how to solve the problems of enterprise data governance using Watson Knowledge Catalog on the Cloud Pak for Data platform. We'll explain how to use governance, data quality and active policy management in order to help your organization protect and govern sensitive data, trace data lineage and manage data lakes. This knowledge will help users quickly discover, curate, categorize and share data assets, data sets, analytical models and their relationships with other members of your organization. It serves as a single source of truth for data engineers, data stewards, data scientists and business analysts to gain self-service access to data they can trust.</p> <p>You will need the Admin role to create a catalog.</p>"},{"location":"watson-knowledge-catalog-admin/#1-set-up-catalog","title":"1. Set up Catalog","text":"<p>NOTE: The default catalog is your enterprise catalog. It is created automatically after you install the Watson Knowledge Catalog service and is the only catalog to which advanced data curation tools apply. The default catalog is governed so that data protection rules are enforced. The information assets view shows additional properties of the assets in the default catalog to aid curation. Any subsequent catalogs that you create can be governed or ungoverned, do not have an information assets view, and supply basic data curation tools.</p> <p>First we'll create a catalog and load some data</p>"},{"location":"watson-knowledge-catalog-admin/#create-the-catalog","title":"Create the catalog","text":"<ul> <li>Go to the upper-left (\u2630) hamburger menu and choose <code>Catalogs</code> -&gt; <code>All catalogs</code>.</li> </ul> <ul> <li>From the Your catalogs page, click the <code>Create catalog</code> button.</li> </ul> <ul> <li>Give your catalog a name, check the <code>Enforce data protection rules</code> checkbox and provide an optional description. Then click the <code>Create</code> button.</li> </ul> <p>Note: Click <code>Ok</code> in the pop up window when selecting the data protection checkbox.</p>"},{"location":"watson-knowledge-catalog-admin/#2-add-data-assets","title":"2. Add Data Assets","text":"<p>There are several ways to add assets to the catalog. We are going to add a local data asset. There are also optional sections to add connection assets below.</p>"},{"location":"watson-knowledge-catalog-admin/#local-data-asset","title":"Local Data Asset","text":"<ul> <li>Click <code>Add to Catalog +</code> in the top right and choose <code>Local files</code>.</li> </ul> <ul> <li>Click the <code>browse</code> link in the 'Select file(s) panel. Browse to the <code>/data/split/applicant_personal_data.csv</code> file to select it. Add an optional description and click the <code>Add</code> button.</li> </ul> <p>NOTE: Stay in the catalog until loading is complete! If you leave the catalog, the incomplete asset will be deleted.</p> <ul> <li>The newly added file will show up under the Browse Assets tab of your catalog:</li> </ul> <p></p>"},{"location":"watson-knowledge-catalog-admin/#optional-add-connection","title":"(Optional) Add Connection","text":"<ul> <li>You can add a connection to various data sources, for example DB2 Warehouse in IBM Cloud, by choosing <code>Add to Catalog +</code> -&gt; <code>Connection</code>:</li> </ul> <ul> <li>Click on the data source type you want to add (for example, <code>Db2 Warehouse</code>).</li> </ul> <ul> <li>Enter the connection details and click <code>Create</code>:</li> </ul> <ul> <li>The connection now shows up in the catalog.</li> </ul> <p>Note: Virtualized data can be added to the Default catalog by someone with Administrator or Editor access to that catalog. There is an option to add <code>Data Virtualization</code> as a connection.</p>"},{"location":"watson-knowledge-catalog-admin/#optional-add-data-from-connection","title":"(Optional) Add Data from Connection","text":"<p>Once you have a connection to a data source, you will be able to add assets from that connection.</p> <ul> <li>Click <code>+Add to Catalog</code> -&gt; <code>Connected asset</code>:</li> </ul> <p></p> <ul> <li>Click Source -&gt; <code>Select source</code>. Browse under <code>DV</code> to you Schema (i.e. UserXYZW) and choose the joined table. Click <code>Select</code>.</li> </ul> <p>A user can now add this to a project like any other asset from a catalog.</p>"},{"location":"watson-knowledge-catalog-admin/#2-add-collaborators-and-review-data","title":"2. Add Collaborators and Review Data","text":"<ul> <li>Under the Access Control tab you can click <code>Add Collaborator</code> to give other users access to your catalog.</li> </ul> <ul> <li> <p>You can search for a user by entering their name in the <code>Collaborators</code> field. Click on the name to select the user., and click <code>Add</code>.</p> </li> <li> <p>You can choose a role for the user - <code>Admin</code>, <code>Editor</code>, or <code>Viewer</code>. Then click the <code>Add</code> button.</p> </li> </ul> <p></p> <ul> <li>To access data in the catalog, click on the name of the data.</li> </ul> <p></p> <ul> <li>An overview of the data will open with metadata and Governance artifacts.</li> </ul> <p></p> <ul> <li>Click on the <code>Asset</code> tab to see a preview of the first 1000 rows.</li> </ul> <p></p> <ul> <li>You can click the <code>Review</code> tab and rate the data, as well as comment on it, to provide feedback to consumers of the data.</li> </ul> <p></p>"},{"location":"watson-knowledge-catalog-admin/#3-add-categories","title":"3. Add categories","text":"<p>The fundamental abstraction in Watson Knowledge Catalog is the Category. A category is analogous to a folder. You can add categories as needed, or you can import them in .csv format.</p>"},{"location":"watson-knowledge-catalog-admin/#import-categories-optional","title":"Import categories (optional)","text":"<p>To import categories with unique names, you will need to be comfortable with running a command in a terminal window. Please skip this if you are not familiar with that process.</p> <ul> <li>All category names are global in scope, so you'll need to import a file with unique names. Go to where you cloned or downloaded this repository, and navigate to the file <code>data/wkc/glossary-organize-categories.csv</code>. Run the script <code>data/wkc/prepend-user-tag.py</code> using your intials or some other tag in order to create a unique file. For example, I might run <code>./prepend-user-tag -T scottda</code>. If you do not add a tag with the <code>-T</code> parameter, a unique file with unique Category names will be generated with a python time.time() string.</li> </ul> <p></p> <ul> <li>Import a category for your assets by going to the upper-left (\u2630) hamburger menu, choose <code>Governance</code> -&gt; <code>Categories</code>, then the click the <code>Add category</code> button and choose <code>Import from file</code>. </li> </ul> <p></p> <ul> <li>Click the <code>Add file</code> and navigate to where you cloned/downloaded the workshop repository, choosing the file that you have created using the <code>prepend-user-tag.py</code> script, i.e <code>data/wkc/scottda-glossary-organize-categories.csv</code> would be the file I created by running <code>./prepend-user-tag.py -T scottda</code>. Click the <code>Next</code> button.</li> </ul> <p></p> <ul> <li>Under <code>Select merge option</code> choose <code>Replace all values</code> and click <code>Import</code>.</li> </ul> <p></p> <ul> <li> <p>You will see \"The import completed succesfully\" when it is completed.</p> </li> <li> <p>In this way, you can import Categories, Business Terms, Classifications, Policies, etc. to populate your governance catalogs.</p> </li> </ul>"},{"location":"watson-knowledge-catalog-admin/#add-category-manually","title":"Add category manually","text":"<p>NOTE: Categories, Business Terms, Data Classes, and othe Governance artifacts are global in scope. When you are asked to create one, pre-pend your initials or some unique tag, or it will fail. For example, below I would use <code>scottda-Personal Data</code> in place of <code>XXX-Personal Data</code>.</p> <p>In addition to importing, you can manually create categories. Add a category for your assets by going to the upper-left (\u2630) hamburger menu, choose <code>Governance</code> -&gt; <code>Categories</code>, then click the <code>Add category</code> button and then <code>New category</code>.</p> <p></p> <ul> <li>Give your category a name pre-pended with initials or a unique tag, such as XXX-Personal Data, and an optional description, and then click the <code>Save</code> button.</li> </ul> <p></p> <ul> <li>Now, if you hit the <code>Create category</code> link on the Personal Data category screen under Subcategories, you can create a subcategory, such as Residence Information.</li> </ul> <p></p> <ul> <li>For the Personal Data category you can select a Type, such as <code>Business term</code>.</li> </ul> <p></p> <ul> <li>We can also create classifications for assets, similar to Confidential, Personally Identifiable Information, or Sensitive Personal Information in a similar way, by going to the upper-left (\u2630) hamburger menu, choose <code>Governance</code> -&gt; <code>Classifications</code>.</li> </ul> <p></p> <ul> <li>Click on the <code>Create classification</code> button on the top right and then <code>New classification</code> from the drop down menu. These classifications can then be added to your category as a Type:</li> </ul> <p></p>"},{"location":"watson-knowledge-catalog-admin/#4-add-data-classes","title":"4. Add data classes","text":"<p>NOTE: Categories, Business Terms, Data Classes, and othe Governance artifacts are global in scope. When you are asked to create one, pre-pend your initials or some unique tag, or it will fail. For example, below I would use <code>scottda-alphanumeric</code> in place of <code>XXX-alphanumeric</code>.</p> <p>When you profile your assets, a data class will be inferred from the contents where possible. We'll see more on this later. You can also add your own data classes.</p> <ul> <li>Add a data class for your assets by going to the upper-left (\u2630) hamburger menu, choose <code>Governance</code> -&gt; <code>Data classes</code>, then click the <code>Add data class</code> button and then the<code>New data class</code> option from the drop down menu.</li> </ul> <p></p> <ul> <li>Give your new data class a name pre-pended with initials or a tag, i.e. XXX-alphanumeric, and then click <code>Change</code> for Primary category.</li> </ul> <p></p> <ul> <li>Choose the Personal Data primary category and click <code>Add</code>.</li> </ul> <p></p> <ul> <li> <p>Now you can click <code>Save as draft</code>.</p> </li> <li> <p>Once the data class is created, we can optionally: add Stewards for this class, and associate classifications and business terms. When you are ready, click the <code>Publish</code> button and again <code>Publish</code> in the pop up window.</p> </li> </ul> <p></p> <ul> <li> <p>Now let's add that data class to a column in our applicant_personal_data.csv asset.</p> </li> <li> <p>Go back to the catalog you created earlier (i.e CreditDataCatalog) and open it ((\u2630) hamburger menu <code>Catalogs</code> -&gt; <code>All catalogs</code> and choose <code>CreditDataCatalog</code>). Under the Browse assets tab, click on the data set applicant_personal_data.csv, and then the <code>Asset</code> tab, to get the column/row preview. Find the CustomerID column and click the down arrow next to \"Customer Number\" and then View all:</p> </li> </ul> <p></p> <ul> <li>In the window that opens, search for your newly created data class, alphanumeric and click it when it returns in the search. Then click the <code>Select</code> button.</li> </ul> <p></p>"},{"location":"watson-knowledge-catalog-admin/#5-add-business-terms","title":"5. Add Business terms","text":"<p>NOTE: Categories, Business Terms, Data Classes, and othe Governance artifacts are global in scope. When you are asked to create one, pre-pend your initials or some unique tag, or it will fail. For example, below I would use <code>scottda-Contact Information</code> in place of <code>XXX-Contact Information</code>.</p> <p>You can use Business terms to standardize definitions of business concepts so that your data is described in a uniform and easily understood way across your enterprise.</p> <p>You already saw how to create a category and make it a business term. You can also create the business term as it's own entity.</p> <ul> <li>From the upper-left (\u2630) hamburger menu, choose <code>Governance</code> -&gt; <code>Business terms</code>:</li> </ul> <p></p> <ul> <li>Click on the upper-right <code>Add business term</code> button and then the <code>New business term</code> option in the drop down menu.</li> </ul> <p></p> <ul> <li>Give the new Business term a name pre-pended with initials or a tag, such as XXX-Contact Information and optional description. Click <code>Change</code> under Primary category and choose Personal data, then Click the <code>Save as draft</code> button.</li> </ul> <p></p> <ul> <li>A window will come up once the term is created. You can see a rich set of options for creating related terms and adding other metadata. For now, click <code>Publish</code> to make this term available to users of the platform. Go ahead and click <code>Publish</code> on the pop up confirmation window.</li> </ul> <p></p> <ul> <li>Go back to the catalog you created earlier (i.e CreditDataCatalog) and open it ((\u2630) hamburger menu <code>Catalog</code> -&gt; <code>All catalogs</code> and choose <code>CreditDataCatalog</code>). Under the Browse assets tab, click on the data set applicant_personal_data.csv, and then the <code>Asset</code> tab, to get the column/row preview. Find the Email column and click the Column information icon (looks like an \"eye\").</li> </ul> <p></p> <ul> <li>In the window that opens, click the edit icon (looks like a \"pencil\") next to Business terms :</li> </ul> <p></p> <ul> <li>Enter XXX-Contact Information (your uniquely named term such as scottda-ContactInfo) term you created earlier under Business terms and the term will be searched for. Click on the <code>Contact Information</code> term that is found, and click <code>Apply</code>:</li> </ul> <p></p> <ul> <li> <p>Click <code>Close</code> in that window once the term has been applied. Now, do the same thing to add the Contact Information Business term to the Telephone column.</p> </li> <li> <p>You will now be able to search for these terms from within the platform. For example, going back to your top level CreditDataCatalog, in the search bar with the comment \"What assets are you searching for?\" enter your unique Contact Information term: <p></p> <ul> <li>The applicant_personal_data.csv data set will show up, since it contains columns tagged with the Contact Infomation business term.</li> </ul>"},{"location":"watson-knowledge-catalog-admin/#6-add-rules-for-policies","title":"6. Add rules for policies","text":"<p>We can now create rules to control how a user can access data.</p> <p>NOTE: Workshop teammates can simply reuse 1 term to associate with a rule, i.e. CustomerID, or you can proceed below to create a uniquely named one.</p> <ul> <li>Create a business term called XXX-CustomerID, or re-use one of your workshop teammates buisness terms for this expercise. Assign it to your CustomerID column in the data set using the instructions above. See below if you need details, but try it yourself first, and skip to Adding a rule below if you do not need a reminder.</li> </ul>"},{"location":"watson-knowledge-catalog-admin/#how-to-create-a-business-term-review","title":"How to create a Business term review","text":"<ul> <li> <p>From the upper-left (\u2630) hamburger menu, choose <code>Governance</code> -&gt; <code>Business terms</code>.</p> </li> <li> <p>Click on the upper-right <code>Add business term</code> button and then the <code>New business term</code> option in the drop down menu.</p> </li> <li> <p>Give the new Business term the name XXX-CustomerID and optional description. Click <code>Change</code> under Primary category and choose Personal data, then Click the <code>Save as draft</code> button. In the next window, click <code>Publish</code>.</p> </li> <li> <p>Go back to the catalog you created earlier (i.e CreditDataCatalog) and open it ((\u2630) hamburger menu <code>Catalog</code> -&gt; <code>All catalogs</code> and choose <code>CreditDataCatalog</code>). Under the Browse assets tab, click on the data set applicant_personal_data.csv, and then the <code>Asset</code> tab, to get the column/row preview. Find the CustomerID column and click the Column information icon (looks like an \"eye\").</p> </li> <li> <p>In the window that opens, click the edit icon (looks like a \"pencil\") next to Business terms .</p> </li> <li> <p>Enter CustomerID under Business terms and the term will be searched for. Click on the <code>CustumerID</code> term that is found, and click <code>Apply</code>. Then close the pop up window.</p> </li> </ul>"},{"location":"watson-knowledge-catalog-admin/#adding-a-rule","title":"Adding a rule","text":"<ul> <li> <p>From the upper-left (\u2630) hamburger menu, choose <code>Governance</code> -&gt; <code>Rules</code>.</p> </li> <li> <p>Click the <code>Add rule</code> button on the top right and then select the  <code>New rule</code> option from the drop down menu.</p> </li> <li> <p>In the 'Create a new rule' page, select the <code>Data protection rule</code> option.</p> </li> </ul> <p></p> <ul> <li> <p>Give your rule a unique XXX-Name, leave the Type set to <code>Access</code>, and add a Business definition.</p> </li> <li> <p>Under Rule builder &gt; Condition1: For the <code>If</code> condition, select Business term Contains any CustomerID. Under Action, for the <code>then</code> panel, select mask data in columns containing alphanumeric. Choose the tile for <code>Substitute</code>, which will make a non-identifiable hash. This obscures the actual CustomerID, but allows actions like database joins to still work. Click the <code>Create rule</code> button.</p> </li> </ul> <p></p> <ul> <li> <p>Now if we go back to our applicant_personal_data.csv asset in the catalog at the CustomerID column, it will look the same as before. But a non-admin user will see the \"lock\" icon and see that the customerID has now been substituted with a hash value.</p> </li> <li> <p>To add a rule to Obfuscate data, create a new data class called Age. See the instructions above if needed, don't forget to publish the class.</p> </li> <li> <p>Back in the CreditDataCatalog, under the applicant_personal_data.csv asset, go to the <code>Overview</code> tab and scroll to the Age column. Click the \"down arrow\" and you can see that the data has been inferred to be classified as a Code. Change the classifier by clicking <code>View all</code>.</p> </li> </ul> <p></p> <ul> <li>Now change the classifier by starting to type Age. When this comes up in the search, select it and then click the <code>Select</code> button.</li> </ul> <p></p> <ul> <li>Following the prior instructions, you can build a new data protection rule to Obfuscate this Age column.</li> </ul> <p></p> <ul> <li>And now when that column is viewed by a non-admin user, it will have data that is replaced with similarly formatted data.</li> </ul>"},{"location":"watson-knowledge-catalog-admin/#conclusion","title":"Conclusion","text":"<p>In this lab, we learned how to:</p> <ul> <li>Set up Catalog and Data</li> <li>Add collaborators and control access</li> <li>Add categories</li> <li>Add data classes</li> <li>Add Business terms</li> <li>Add rules for policies</li> </ul>"},{"location":"watson-knowledge-catalog-user/","title":"Watson Knowledge Catalog for Users","text":"<p>This exercise demonstrates how to solve the problems of enterprise data governance using Watson Knowledge Catalog on the Cloud Pak for Data platform. We'll explain how to use governance, data quality and active policy management in order to help your organization protect and govern sensitive data, trace data lineage and manage data lakes. This knowledge will help users quickly discover, curate, categorize and share data assets, data sets, analytical models and their relationships with other members of your organization. It serves as a single source of truth for data engineers, data stewards, data scientists and business analysts to gain self-service access to data they can trust.</p>"},{"location":"watson-knowledge-catalog-user/#1-find-the-right-data","title":"1. Find the Right Data","text":"<p>We need to find the right data and business information related to the Mortgage Default analysis project. You can use the global search to search across catalogs, projects and the business glossary to find all assets that you may be interested in.</p> <ul> <li>Enter the words <code>mortgage</code> in the global search area and press the enter key to start finding what you need. Place your cursor inside the global search area next to the word mortgage and press the enter key:</li> </ul> <p></p> <ul> <li> <p>The search returns all data and information assets related to the search criteria across all catalogs, projects and governance artifacts. Scroll down through the list to take a closer look at what was found.</p> </li> <li> <p>You can further refine your search results by using the filters supplied by type, owner, and modification time.</p> </li> </ul> <p></p> <ul> <li> <p>All data assets across catalogs meeting the criteria are displayed. This is the data we are looking for.</p> </li> <li> <p>The connection to the Analytics Data Warehouse and the 4 mortgage tables are what the project team requested; Mortgage Default, Applicant, Property and Customer are all in the Enterprise catalog.</p> </li> <li> <p>However, before we proceed to the catalog we need to also find all the business information related to the project to review the terms and content of the data and identify if there are any policies and rules set by the business that the project team needs to be aware of and adhere to.</p> </li> <li> <p>From the menu on the left, select <code>Business terms</code>.</p> </li> </ul> <p></p> <ul> <li> <p>Notice that the governance team has been hard at work and has defined 25 business terms related to the mortgage data the project will be using. Also notice that they are tagged with the key word Mortgage to easily find them</p> </li> <li> <p>Scroll down the list of business terms to view them all.</p> </li> <li> <p>Note that the Email Address, Phone Number and Social Security Number have all been tagged as Sensitive information.</p> </li> </ul> <p></p> <ul> <li> <p>Click on the <code>All</code> all button to get ready for the next search task.</p> </li> <li> <p>Click on the Category type from the list to refine the results.</p> </li> </ul> <p></p> <ul> <li>Notice that there is a Mortgage Default Analysis category defined that contains all the business information related to the project and a category named Sensitive Information that is a subcategory. This is a good indication that the mortgage data being used by the project contains sensitive information that needs to be protected.</li> </ul>"},{"location":"watson-knowledge-catalog-user/#2-understand-the-data","title":"2. Understand the Data","text":"<p>The best data is data that is fully understood and trusted.</p> <p>You can be confident in your data when you know where it comes from, that it complies to a set of policies and rules that address data privacy regulations and that it is clean and conforms to data quality policies, rules and standards, and that others have used it and trust it and are willing to share that information, to ensure you can produce meaningful and accurate analytical and AI results that will benefit better business outcomes.</p>"},{"location":"watson-knowledge-catalog-user/#understand-data-policies-and-rules","title":"Understand Data Policies and Rules","text":"<p>In this section you will use the Business glossary to gain a deeper understanding of the business terminology defined by the data steward and the governance team responsible for establishing policies and rules to govern and protect the data. Since we see that there is an indication of sensitive information let\u2019s take a closer look at the Sensitive Information category content.</p> <ul> <li>Click on the Sensitive Information category from the list.</li> </ul> <p></p> <ul> <li>The description of the category clearly states that this subcategory contains references to business terms that relate to data that will be used by the project team that need to be governed by data protection rules. It also shows the related governance artifacts. Click on the `Social Security Number' artifact.</li> </ul> <p></p> <ul> <li>Click on the <code>Related content</code> tab and click on the <code>Protection of Sensitive Information</code> policy. </li> </ul> <p></p> <ul> <li>Click on the <code>Protection of Sensitive Information</code> policy. See the rules, data protection rules, and related artifacts associated with the policy.</li> </ul> <p></p>"},{"location":"watson-knowledge-catalog-user/#3-understand-the-data-content","title":"3. Understand the Data Content","text":"<p>You have gained an understanding of the policies and rules and information related to sensitive data and validated and trust the data quality. In this section we will go to the Enterprise catalog, which is where we identified all the data we need resides, and use all the features it provides to gain an even better understanding of the data content and have even more confidence in the data based on what others are saying and by utilizing the AI assisted recommendations, automatic profiling and additional data content statistics provided.</p> <ul> <li> <p>Click the (\u2630) hamburger menu in the upper left corner and click <code>Catalog</code> -&gt; <code>All catalogs</code></p> </li> <li> <p>Click on the <code>Enterprise</code> catalog.</p> </li> </ul> <p>Watson Knowledge Catalog provides suggested assets to you based on recommendations using AI, things you might be interested in based on your past viewing history. Notice that it is already recommending Mortgage data to you based on your past searches. It also keeps track of what\u2019s hot and Highly Rated based on reviews and ranks them in order of their rating highest to lowest.</p> <ul> <li>Click on the <code>Highly Rated</code> section to see what\u2019s hot:</li> </ul> <p></p> <ul> <li> <p>Notice that the Analytics Data Warehouse, MORTGAGE_APPLICANT and MORTGAGE_CUSTOMER tables have been reviewed and are rated quite high. A good indication of their quality and usability.</p> </li> <li> <p>Lastly, Watson Knowledge Catalog keeps track of what\u2019s new that has been Recently Added since the last time you visited the catalog. This are all means to help you find and understand the data more quickly and easily.</p> </li> <li> <p>Click on the <code>Recently Added</code> section to see what\u2019s new.</p> </li> <li> <p>Notice that the <code>MORTGAGE_APPLICANT</code> table was the most recent data asset added to the catalog.</p> </li> <li> <p>Click on the <code>MORTGAGE_APPLICANT</code> table to review its content and metadata.</p> </li> <li> <p>You are brought into the Overview section of the MORTGAGE_APPLICANT table. Click on the <code>Asset</code> tab. You may see that Data masking is in progress and that 3 columns are being masked. The asset is being masked by the Protect Sensitive Personal Information data protection rule being enforced by the data governance team. Because you are not authorized to view the sensitive information, the data is being protected.</p> </li> <li> <p>The data that is masked is indicated with a lock icon next to their column names; EMAIL_ADDRESS, PHONE_NUMBER and SOCIAL_SECURITY_NUMBER. Scroll to the right to view all the masked columns.</p> </li> <li> <p>Click on the lock icon on the <code>EMAIL_ADDRESS</code> column to view the data protection message.</p> </li> </ul> <p></p> <ul> <li> <p>Click on the Review tab to read the review. The table has a 5 star rating with a very positive review. Reviews can be written by anyone who has access to the catalog and the asset to notify and inform others of the content and usability of the data.</p> </li> <li> <p>Click on the Profile tab to view the data profile.</p> </li> <li> <p>As data assets are discovered and added to the catalog they are automatically profiled and classified to give end users a more in-depth understanding of the data content, quality and usability. Data classifications are used to identify what type of data it is and to autonomously enforce data protect rules to mask sensitive data, like you just saw.</p> </li> <li> <p>Scroll to the right to view the other columns. Notice that the protected data does not have any profile information displayed.</p> </li> </ul> <p></p> <ul> <li>Lineage is captured for every data asset in a catalog. It keeps track of where it came from, any updates or changes that have been made to its metadata and any movement of the data outside of the catalog. Click on the <code>Activities</code> button to view the data asset lineage.</li> </ul> <p></p> <ul> <li>Click on <code>See details</code> on any node to see the detail pane on the right. Do this for every node in the lineage graph to see what is tracked.</li> </ul>"},{"location":"watson-knowledge-catalog-user/#conclusion","title":"Conclusion","text":"<p>In this section we covered several aspects of data organization and governance. We've seen:</p> <ul> <li>How data is cataloged.</li> <li>How to search for data.</li> <li>How policies and rules can be applied to protect and govern sensitive data.</li> </ul>"}]}